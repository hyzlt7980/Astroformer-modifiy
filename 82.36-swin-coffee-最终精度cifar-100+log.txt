Á¨¨‰∏Ä‰ªΩ‰ª£Á†ÅÊòØË∑ë300 epochÔºåÁÑ∂ÂêéËôöÁ∫ø‰∏ãËæπÁöÑÁ¨¨‰∫å‰ªΩ‰ª£Á†ÅÂú®300 epochÁöÑÂü∫Á°Ä‰∏äÂÜçË∑ë115epoch‰Ωú‰∏∫Á≤æÁªÜÂæÆË∞ÉÔºåLogÂú®ÊúÄ‰∏ãÊñπ


import math
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, DistributedSampler
import os
import torch.distributed as dist
import random
import numpy as np
import time
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.optim.lr_scheduler import LambdaLR

# ÂÖºÂÆπ PyTorch Êñ∞ÊóßÁâàÊú¨ÁöÑ AMP
try:
    from torch.amp import autocast, GradScaler
except ImportError:
    from torch.cuda.amp import autocast, GradScaler

try:
    from timm.layers import ClassifierHead, DropPath, LayerNorm, Mlp, Attention, create_conv2d, to_2tuple
    from timm.models.swin_transformer import SwinTransformerBlock as SwinBlock
    from timm.data.mixup import Mixup
    from timm.loss import SoftTargetCrossEntropy
except ImportError:
    print("Error: timm not installed. Please run 'pip install timm'")
    exit(1)

# =================================================================================
# SECTION 1: Swin-Coffee Êû∂ÊûÑ (‰øùÁïô Noise, Denoise, CBAM)
# =================================================================================

class CBAM(nn.Module):
    def __init__(self, in_planes, ratio=16):
        super().__init__()
        self.ca = nn.Sequential(
            nn.AdaptiveAvgPool1d(1),
            nn.Conv1d(in_planes, in_planes // ratio, 1, bias=False),
            nn.ReLU(),
            nn.Conv1d(in_planes // ratio, in_planes, 1, bias=False),
            nn.Sigmoid()
        )
        self.sa = nn.Sequential(
            nn.Conv2d(2, 1, 7, padding=3, bias=False),
            nn.Sigmoid()
        )

    def forward(self, x, H, W):
        B, H, W, C = x.shape
        x_flat = x.view(B, H * W, C)
        x_p = x_flat.transpose(1, 2) 
        attn_c = self.ca(x_p).transpose(1, 2) 
        x_flat = x_flat * attn_c
        x_s = x_flat.view(B, H, W, C).permute(0, 3, 1, 2) 
        avg_out = torch.mean(x_s, dim=1, keepdim=True)
        max_out, _ = torch.max(x_s, dim=1, keepdim=True)
        attn_s = self.sa(torch.cat([avg_out, max_out], dim=1))
        x_s = x_s * attn_s
        return x_s.permute(0, 2, 3, 1)

class EnhancedDisruptBlock(nn.Module): 
    def __init__(self, dim, disturb_intensity=0.1): 
        super().__init__()
        self.dim = dim
        self.disturb_intensity = disturb_intensity
        
    def _disturb_features(self, x):
        if not self.training or random.random() > 0.5: return x
        x_perm = x.permute(0, 3, 1, 2)
        intensity = self.disturb_intensity * (0.8 + 0.6 * torch.rand(1).item())
        x_fft = torch.fft.rfft2(x_perm.float(), norm="ortho")
        mask = torch.ones_like(x_fft)
        h, w = x_fft.shape[2], x_fft.shape[3]
        h_l, w_l = int(h * intensity), int(w * intensity)
        if h_l > 0 and w_l > 0:
            h_s, w_s = random.randint(0, h-h_l), random.randint(0, w-w_l)
            mask[:, :, h_s:h_s+h_l, w_s:w_s+w_l] = 0
        x_disturbed = torch.fft.irfft2(x_fft * mask, s=(x_perm.shape[2], x_perm.shape[3]), norm="ortho").type_as(x)
        return x_disturbed.permute(0, 2, 3, 1)
    def forward(self, x): return self._disturb_features(x)

class DenoisingDisruptionBlock(nn.Module):
    def __init__(self, dim, num_heads, noise_level=0.1): 
        super().__init__()
        self.norm1 = LayerNorm(dim)
        self.attn = Attention(dim, num_heads=num_heads)
        self.norm2 = LayerNorm(dim)
        self.mlp = Mlp(in_features=dim, hidden_features=dim*4)
        self.noise_level = noise_level
        self.denoise_loss_fn = nn.MSELoss()
        self.denoise_proj = nn.Linear(dim, dim)
        
    def forward(self, x, epoch=None):
        x_clean = x
        # Ê†∏ÂøÉÔºö30ËΩÆÂêéÂºÄÂêØÂô™Â£∞
        if self.training and (epoch is not None and epoch >= 30): 
            x_noisy = x + torch.randn_like(x) * self.noise_level
        else:
            x_noisy = x 
        x_p = x_noisy + self.attn(self.norm1(x_noisy))
        x_p = x_p + self.mlp(self.norm2(x_p))
        if self.training: 
            return x_p, self.denoise_loss_fn(self.denoise_proj(x_p), x_clean.detach())
        return x_p

class AstroformerV4(nn.Module):
    def __init__(self, img_size=64, num_classes=100, embed_dims=(96, 192, 384, 768), depths=(2, 2, 6, 2)):
        super().__init__()
        self.stem = create_conv2d(3, embed_dims[0], 3, stride=2, padding=1)
        dpr = [x.item() for x in torch.linspace(0, 0.2, sum(depths))]
        self.stages = nn.ModuleList()
        in_c = embed_dims[0]
        resolutions = [img_size // 2, img_size // 4, img_size // 8, img_size // 16]
        for i in range(4):
            stride = 1 if i == 0 else 2
            curr_res = resolutions[i]
            res_tuple = (curr_res, curr_res)
            blocks = nn.ModuleList([
                SwinBlock(dim=embed_dims[i], input_resolution=res_tuple, num_heads=4 * (2**i) if i < 3 else 24, 
                          window_size=4, shift_size=0 if j%2==0 else 2, drop_path=dpr[sum(depths[:i])+j])
                for j in range(depths[i])
            ])
            down = nn.Identity()
            if i > 0:
                down = nn.Sequential(create_conv2d(in_c, embed_dims[i], 3, stride=stride, padding=1), nn.BatchNorm2d(embed_dims[i]), nn.GELU())
            self.stages.append(nn.ModuleDict({
                'down': down, 'blocks': blocks, 'cbam': CBAM(embed_dims[i]),
                'disturb': EnhancedDisruptBlock(embed_dims[i], disturb_intensity=0.1)
            }))
            in_c = embed_dims[i]
        self.final_denoise = DenoisingDisruptionBlock(embed_dims[3], 24, noise_level=0.1)
        self.norm = LayerNorm(embed_dims[3])
        self.head = nn.Linear(embed_dims[3], num_classes)

    def forward(self, x, epoch=None):
        x = self.stem(x).permute(0, 2, 3, 1) 
        for i, s in enumerate(self.stages):
            if i > 0:
                x = s['down'](x.permute(0, 3, 1, 2)).permute(0, 2, 3, 1)
            B, H, W, C = x.shape
            for blk in s['blocks']: x = blk(x)
            x = s['cbam'](x, H, W)
            # Ê†∏ÂøÉÔºö30ËΩÆÂêéÂºÄÂêØFFTÊâ∞Âä®
            if epoch is not None and epoch >= 30: x = s['disturb'](x)
        x = self.norm(x)
        out = self.final_denoise(x.view(B, H*W, C), epoch)
        feat, d_loss = out if isinstance(out, tuple) else (out, None)
        logits = self.head(feat.mean(dim=1))
        return (logits, d_loss) if self.training and d_loss is not None else logits

# =================================================================================
# SECTION 2: ËÆ≠ÁªÉÈÄªËæë
# =================================================================================

def build_scheduler(optimizer, warmup_epochs=20, max_epochs=300):
    def lr_lambda(epoch):
        if epoch < warmup_epochs: return float(epoch + 1) / warmup_epochs
        return 0.5 * (1. + math.cos(math.pi * (epoch - warmup_epochs) / (max_epochs - warmup_epochs)))
    return LambdaLR(optimizer, lr_lambda)

def main():
    if 'RANK' in os.environ:
        dist.init_process_group(backend="nccl")
        rank, local_rank = int(os.environ['RANK']), int(os.environ['LOCAL_RANK'])
    else: rank, local_rank = 0, 0
    torch.cuda.set_device(local_rank)
    device = torch.device(f"cuda:{local_rank}")

    BS = 128
    transform_train = transforms.Compose([
        transforms.Resize((64,64)), transforms.RandomHorizontalFlip(),
        transforms.RandAugment(num_ops=2, magnitude=9), transforms.ToTensor(),
        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),
        transforms.RandomErasing(p=0.25)
    ])
    transform_val = transforms.Compose([
        transforms.Resize((64,64)), transforms.ToTensor(),
        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))
    ])

    train_data = torchvision.datasets.CIFAR100('./data', train=True, download=True, transform=transform_train)
    val_data = torchvision.datasets.CIFAR100('./data', train=False, download=True, transform=transform_val)
    train_loader = DataLoader(train_data, batch_size=BS, sampler=DistributedSampler(train_data), num_workers=8, pin_memory=True)
    val_loader = DataLoader(val_data, batch_size=BS, sampler=DistributedSampler(val_data, shuffle=False), num_workers=8)

    mixup_fn = Mixup(mixup_alpha=0.4, cutmix_alpha=0.4, prob=1.0, switch_prob=0.5, mode='batch', label_smoothing=0.1, num_classes=100)
    model_ddp = DDP(AstroformerV4().to(device), device_ids=[local_rank], find_unused_parameters=True)
    optimizer = optim.  (model_ddp.parameters(), lr=1e-3, weight_decay=0.05)
    scheduler = build_scheduler(optimizer, warmup_epochs=20, max_epochs=300)
    scaler = GradScaler()
    criterion = SoftTargetCrossEntropy()

    best_acc = 0.0
    start_time = time.time()
    
    # Ëá™Âä®ÊÅ¢Â§çÈÄªËæë
    if os.path.exists('last_checkpoint_final.pth'):
        if rank == 0: print("==> Resuming from checkpoint...")
        ckpt = torch.load('last_checkpoint_final.pth', map_location=device)
        model_ddp.module.load_state_dict(ckpt['model_state_dict'])
        optimizer.load_state_dict(ckpt['optimizer_state_dict'])
        scheduler.load_state_dict(ckpt['scheduler_state_dict'])
        best_acc = ckpt['best_acc']
        start_epoch = ckpt['epoch'] + 1
    else:
        start_epoch = 0

    for epoch in range(start_epoch, 300):
        if dist.is_initialized(): train_loader.sampler.set_epoch(epoch)
        model_ddp.train()
        avg_loss, steps = 0, 0
        
        for inputs, targets in train_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            inputs, targets = mixup_fn(inputs, targets)
            
            # [ÂÖ≥ÈîÆ‰øÆÂ§ç] Ê∑ªÂä† device_type='cuda'
            with autocast(device_type='cuda', dtype=torch.float16):
                out = model_ddp(inputs, epoch)
                logits, d_loss = out if isinstance(out, tuple) else (out, None)
                loss = criterion(logits, targets)
                if d_loss is not None: loss += 0.1 * d_loss
            
            # Loss NaN Ê£ÄÊü•
            if not math.isfinite(loss.item()):
                print(f"Loss is {loss.item()}, stopping training")
                exit(1)

            optimizer.zero_grad()
            scaler.scale(loss).backward()
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model_ddp.parameters(), max_norm=1.0)
            scaler.step(optimizer)
            scaler.update()
            avg_loss += loss.item()
            steps += 1
        
        scheduler.step()
        if rank == 0:
            current_lr = optimizer.param_groups[0]['lr']
            model_ddp.eval()
            correct, total = 0, 0
            with torch.no_grad():
                for inputs, targets in val_loader:
                    inputs, targets = inputs.to(device), targets.to(device)
                    logits = model_ddp(inputs)
                    if isinstance(logits, tuple): logits = logits[0]
                    _, predicted = logits.max(1)
                    total += targets.size(0)
                    correct += predicted.eq(targets).sum().item()
            acc = 100. * correct / total
            if acc > best_acc:
                best_acc = acc
                torch.save(model_ddp.module.state_dict(), 'best_swin_revenge.pth')
            
            torch.save({
                'epoch': epoch, 'best_acc': best_acc, 
                'model_state_dict': model_ddp.module.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(), 
                'scheduler_state_dict': scheduler.state_dict()
            }, 'last_checkpoint_final.pth')

            print(f"Ep {epoch+1:03d} | Loss: {avg_loss/steps:.4f} | LR: {current_lr:.5f} | Acc: {acc:.2f}% | Best: {best_acc:.2f}% | Time: {(time.time()-start_time)/60:.1f}m")

    if dist.is_initialized(): dist.destroy_process_group()

if __name__ == '__main__': main()


-------------------------



import math, torch, torch.nn as nn, torch.nn.functional as F, torch.optim as optim
import torchvision, torchvision.transforms as transforms
from torch.utils.data import DataLoader, DistributedSampler
import os, torch.distributed as dist, random, numpy as np, time
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.optim.lr_scheduler import CosineAnnealingLR

# ÂÖºÂÆπÊÄßÁªÑ‰ª∂
try:
    from torch.amp import autocast, GradScaler
except ImportError:
    from torch.cuda.amp import autocast, GradScaler

try:
    from timm.layers import LayerNorm, Mlp, Attention, create_conv2d
    from timm.models.swin_transformer import SwinTransformerBlock as SwinBlock
    from timm.data.mixup import Mixup
    from timm.loss import SoftTargetCrossEntropy
except ImportError:
    exit(1)

# =================================================================================
# SECTION 1: Êû∂ÊûÑÂÆö‰πâ (‰øùÊåÅ Code B Êû∂ÊûÑ)
# =================================================================================

class CBAM(nn.Module):
    def __init__(self, in_planes, ratio=16):
        super().__init__()
        self.ca = nn.Sequential(
            nn.AdaptiveAvgPool1d(1),
            nn.Conv1d(in_planes, in_planes // ratio, 1, bias=False),
            nn.ReLU(),
            nn.Conv1d(in_planes // ratio, in_planes, 1, bias=False),
            nn.Sigmoid()
        )
        self.sa = nn.Sequential(
            nn.Conv2d(2, 1, 7, padding=3, bias=False),
            nn.Sigmoid()
        )

    def forward(self, x, H, W):
        B, H, W, C = x.shape
        x_flat = x.view(B, H * W, C)
        x_p = x_flat.transpose(1, 2) 
        attn_c = self.ca(x_p).transpose(1, 2) 
        x_flat = x_flat * attn_c
        x_s = x_flat.view(B, H, W, C).permute(0, 3, 1, 2) 
        avg_out = torch.mean(x_s, dim=1, keepdim=True)
        max_out, _ = torch.max(x_s, dim=1, keepdim=True)
        attn_s = self.sa(torch.cat([avg_out, max_out], dim=1))
        x_s = x_s * attn_s
        return x_s.permute(0, 2, 3, 1)

class EnhancedDisruptBlock(nn.Module): 
    def __init__(self, dim, disturb_intensity=0.05):
        super().__init__()
        self.dim = dim
        self.disturb_intensity = disturb_intensity
        
    def _disturb_features(self, x):
        if not self.training or random.random() > 0.5: return x
        x_perm = x.permute(0, 3, 1, 2)
        intensity = self.disturb_intensity * (0.8 + 0.6 * torch.rand(1).item())
        x_fft = torch.fft.rfft2(x_perm.float(), norm="ortho")
        mask = torch.ones_like(x_fft)
        h, w = x_fft.shape[2], x_fft.shape[3]
        h_l, w_l = int(h * intensity), int(w * intensity)
        if h_l > 0 and w_l > 0:
            h_s, w_s = random.randint(0, h-h_l), random.randint(0, w-w_l)
            mask[:, :, h_s:h_s+h_l, w_s:w_s+w_l] = 0
        x_disturbed = torch.fft.irfft2(x_fft * mask, s=(x_perm.shape[2], x_perm.shape[3]), norm="ortho").type_as(x)
        return x_disturbed.permute(0, 2, 3, 1)
    def forward(self, x): return self._disturb_features(x)

class DenoisingDisruptionBlock(nn.Module):
    def __init__(self, dim, num_heads, noise_level=0.1): 
        super().__init__()
        self.norm1 = LayerNorm(dim)
        self.attn = Attention(dim, num_heads=num_heads)
        self.norm2 = LayerNorm(dim)
        self.mlp = Mlp(in_features=dim, hidden_features=dim*4)
        self.noise_level = noise_level
        self.denoise_loss_fn = nn.MSELoss()
        self.denoise_proj = nn.Linear(dim, dim)
        
    def forward(self, x, epoch=None):
        x_clean = x
        if self.training: 
            x_noisy = x + torch.randn_like(x) * self.noise_level
        else:
            x_noisy = x 
        x_p = x_noisy + self.attn(self.norm1(x_noisy))
        x_p = x_p + self.mlp(self.norm2(x_p))
        if self.training: 
            return x_p, self.denoise_loss_fn(self.denoise_proj(x_p), x_clean.detach())
        return x_p

class AstroformerV4(nn.Module):
    def __init__(self, img_size=64, num_classes=100, embed_dims=(96, 192, 384, 768), depths=(2, 2, 6, 2)):
        super().__init__()
        self.stem = create_conv2d(3, embed_dims[0], 3, stride=2, padding=1)
        dpr = [x.item() for x in torch.linspace(0, 0.2, sum(depths))]
        self.stages = nn.ModuleList()
        in_c = embed_dims[0]
        resolutions = [img_size // 2, img_size // 4, img_size // 8, img_size // 16]
        for i in range(4):
            stride = 1 if i == 0 else 2
            curr_res = resolutions[i]
            res_tuple = (curr_res, curr_res)
            blocks = nn.ModuleList([
                SwinBlock(dim=embed_dims[i], input_resolution=res_tuple, num_heads=4 * (2**i) if i < 3 else 24, 
                          window_size=4, shift_size=0 if j%2==0 else 2, drop_path=dpr[sum(depths[:i])+j])
                for j in range(depths[i])
            ])
            down = nn.Identity()
            if i > 0:
                down = nn.Sequential(create_conv2d(in_c, embed_dims[i], 3, stride=stride, padding=1), nn.BatchNorm2d(embed_dims[i]), nn.GELU())
            self.stages.append(nn.ModuleDict({
                'down': down, 'blocks': blocks, 'cbam': CBAM(embed_dims[i]),
                'disturb': EnhancedDisruptBlock(embed_dims[i])
            }))
            in_c = embed_dims[i]
        self.final_denoise = DenoisingDisruptionBlock(embed_dims[3], 24, noise_level=0.1)
        self.norm = LayerNorm(embed_dims[3])
        self.head = nn.Linear(embed_dims[3], num_classes)

    def forward(self, x, epoch=None):
        x = self.stem(x).permute(0, 2, 3, 1) 
        for i, s in enumerate(self.stages):
            if i > 0:
                x = s['down'](x.permute(0, 3, 1, 2)).permute(0, 2, 3, 1)
            B, H, W, C = x.shape
            for blk in s['blocks']: x = blk(x)
            x = s['cbam'](x, H, W)
            x = s['disturb'](x)
        x = self.norm(x)
        out = self.final_denoise(x.view(B, H*W, C), epoch)
        feat, d_loss = out if isinstance(out, tuple) else (out, None)
        logits = self.head(feat.mean(dim=1))
        return (logits, d_loss) if self.training and d_loss is not None else logits

# =================================================================================
# SECTION 2: ‰∏âÈò∂ÊèêÁ∫ØÈÄªËæë
# =================================================================================

def get_params_for_stage(epoch):
    """Ê†πÊçÆÂΩìÂâç epoch ËøîÂõû‰∏âÈò∂ÊÆµÁöÑ Mixup Âíå Âô™Â£∞Âº∫Â∫¶"""
    if epoch < 15:
        # Á¨¨‰∏ÄÈò∂ÊÆµ: ‰øùÊåÅ 0.2 mixup, 0.05 Âô™Â£∞
        return 0.2, 0.050, 0.05
    elif epoch < 65:
        # Á¨¨‰∫åÈò∂ÊÆµ: Âô™Â£∞ÂáèÂçä (0.1 mixup, 0.025 Âô™Â£∞)
        return 0.1, 0.025, 0.02
    else:
        # Á¨¨‰∏âÈò∂ÊÆµ: Âô™Â£∞ÂÜçÂáèÂçä (0.05 mixup, 0.0125 Âô™Â£∞)
        return 0.05, 0.012, 0.01

def main():
    if 'RANK' in os.environ:
        dist.init_process_group(backend="nccl")
        rank, local_rank = int(os.environ['RANK']), int(os.environ['LOCAL_RANK'])
    else: rank, local_rank = 0, 0
    torch.cuda.set_device(local_rank)
    device = torch.device(f"cuda:{local_rank}")

    BS = 128
    transform_train = transforms.Compose([
        transforms.Resize((64,64)), transforms.RandomHorizontalFlip(),
        transforms.RandAugment(num_ops=2, magnitude=9), transforms.ToTensor(),
        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),
        transforms.RandomErasing(p=0.25)
    ])
    transform_val = transforms.Compose([
        transforms.Resize((64,64)), transforms.ToTensor(),
        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))
    ])

    train_data = torchvision.datasets.CIFAR100('./data', train=True, download=True, transform=transform_train)
    val_data = torchvision.datasets.CIFAR100('./data', train=False, download=True, transform=transform_val)
    train_loader = DataLoader(train_data, batch_size=BS, sampler=DistributedSampler(train_data), num_workers=8, pin_memory=True)
    val_loader = DataLoader(val_data, batch_size=BS, sampler=DistributedSampler(val_data, shuffle=False), num_workers=8)

    model = AstroformerV4().to(device)
    model_ddp = DDP(model, device_ids=[local_rank], find_unused_parameters=True)
    
    # ‰ºòÂåñÂô®‰∏éË∞ÉÂ∫¶Âô® (115 ËΩÆÊÄªÈïø)
    optimizer = optim.AdamW(model_ddp.parameters(), lr=1e-5, weight_decay=0.05)
    scheduler = CosineAnnealingLR(optimizer, T_max=115, eta_min=1e-6)
    scaler = GradScaler()
    criterion = SoftTargetCrossEntropy()

    best_acc = 0.0
    start_epoch = 0

    # ==========================================
    # üåü Ê†∏ÂøÉÔºö‰ªé best_polished_final.pth Resume
    # ==========================================
    resume_path = 'best_polished_final.pth'
    last_ckpt_path = 'last_checkpoint_3stage.pth'

    if os.path.exists(last_ckpt_path):
        if rank == 0: print(f"==> Resuming from last 3-stage checkpoint: {last_ckpt_path}")
        ckpt = torch.load(last_ckpt_path, map_location=device)
        model_ddp.module.load_state_dict(ckpt['model_state_dict'])
        optimizer.load_state_dict(ckpt['optimizer_state_dict'])
        scheduler.load_state_dict(ckpt['scheduler_state_dict'])
        start_epoch = ckpt['epoch'] + 1
        best_acc = ckpt['best_acc']
    elif os.path.exists(resume_path):
        if rank == 0: print(f"==> Loading from {resume_path} to start 3-stage polish...")
        state_dict = torch.load(resume_path, map_location=device)
        model_ddp.module.load_state_dict(state_dict, strict=True)
    else:
        if rank == 0: print(f"Error: {resume_path} not found!")
        exit(1)

    start_time = time.time()

    for epoch in range(start_epoch, 115):
        if dist.is_initialized(): train_loader.sampler.set_epoch(epoch)
        
        # --- Âä®ÊÄÅË∞ÉÊï¥ Mixup ÂíåÂô™Â£∞Âº∫Â∫¶ ---
        mix_val, noise_val, smooth_val = get_params_for_stage(epoch)
        
        mixup_fn = Mixup(
            mixup_alpha=mix_val, cutmix_alpha=mix_val, 
            prob=1.0, switch_prob=0.5, mode='batch', 
            label_smoothing=smooth_val, num_classes=100
        )
        
        # Âº∫Âà∂Êõ¥Êñ∞Ê®°ÂûãÂÜÖÁöÑÊâ∞Âä®Âº∫Â∫¶
        for stage in model_ddp.module.stages:
            if 'disturb' in stage:
                stage['disturb'].disturb_intensity = noise_val

        model_ddp.train()
        avg_loss, steps = 0, 0
        
        for inputs, targets in train_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            inputs, targets = mixup_fn(inputs, targets)
            
            with autocast(device_type='cuda', dtype=torch.float16):
                out = model_ddp(inputs, epoch + 300) 
                logits, d_loss = out if isinstance(out, tuple) else (out, None)
                loss = criterion(logits, targets)
                if d_loss is not None: loss += 0.1 * d_loss
            
            optimizer.zero_grad()
            scaler.scale(loss).backward()
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model_ddp.parameters(), max_norm=1.0)
            scaler.step(optimizer)
            scaler.update()
            avg_loss += loss.item()
            steps += 1
        
        scheduler.step()
        
        if rank == 0:
            current_lr = optimizer.param_groups[0]['lr']
            model_ddp.eval()
            correct, total = 0, 0
            with torch.no_grad():
                for inputs, targets in val_loader:
                    inputs, targets = inputs.to(device), targets.to(device)
                    logits = model_ddp(inputs)
                    if isinstance(logits, tuple): logits = logits[0]
                    _, predicted = logits.max(1)
                    total += targets.size(0)
                    correct += predicted.eq(targets).sum().item()
            
            acc = 100. * correct / total
            if acc > best_acc:
                best_acc = acc
                torch.save(model_ddp.module.state_dict(), 'best_polished_ultimate.pth')
            
            torch.save({
                'epoch': epoch, 'best_acc': best_acc,
                'model_state_dict': model_ddp.module.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
            }, last_ckpt_path)
            
            stage_info = "S1" if epoch < 15 else ("S2" if epoch < 65 else "S3")
            print(f"[{stage_info}] Ep {epoch+1:03d} | Mix:{mix_val:.2f} Noise:{noise_val:.3f} | Loss:{avg_loss/steps:.4f} | Acc:{acc:.2f}% | Best:{best_acc:.2f}% | Time:{(time.time()-start_time)/60:.1f}m")

    if dist.is_initialized(): dist.destroy_process_group()

if __name__ == '__main__': main()

-------------------------------------------
base) root@ubuntu22:~# CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --nproc_per_node=4 --master_port=29912 train_swin_coffee_84.py
W0127 23:53:47.043000 5594 site-packages/torch/distributed/run.py:793] 
W0127 23:53:47.043000 5594 site-packages/torch/distributed/run.py:793] *****************************************
W0127 23:53:47.043000 5594 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0127 23:53:47.043000 5594 site-packages/torch/distributed/run.py:793] *****************************************
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verifiedFiles already downloaded and verified

Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
[rank0]:[W127 23:53:53.811731982 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank2]:[W127 23:53:53.811738082 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[W127 23:53:53.811749390 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank3]:[W127 23:53:53.816071114 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
Ep 001 | Loss: 4.4473 | LR: 0.00010 | Acc: 12.64% | Best: 12.64% | Time: 0.2m
Ep 002 | Loss: 4.2246 | LR: 0.00015 | Acc: 17.04% | Best: 17.04% | Time: 0.5m
Ep 003 | Loss: 4.1383 | LR: 0.00020 | Acc: 19.68% | Best: 19.68% | Time: 0.7m
Ep 004 | Loss: 3.9979 | LR: 0.00025 | Acc: 23.52% | Best: 23.52% | Time: 0.9m
Ep 005 | Loss: 3.8540 | LR: 0.00030 | Acc: 28.76% | Best: 28.76% | Time: 1.2m
Ep 006 | Loss: 3.8112 | LR: 0.00035 | Acc: 32.64% | Best: 32.64% | Time: 1.4m
Ep 007 | Loss: 3.6017 | LR: 0.00040 | Acc: 35.68% | Best: 35.68% | Time: 1.6m
Ep 008 | Loss: 3.5588 | LR: 0.00045 | Acc: 38.88% | Best: 38.88% | Time: 1.9m
Ep 009 | Loss: 3.4848 | LR: 0.00050 | Acc: 41.88% | Best: 41.88% | Time: 2.1m
Ep 010 | Loss: 3.5254 | LR: 0.00055 | Acc: 42.96% | Best: 42.96% | Time: 2.3m
Ep 011 | Loss: 3.3471 | LR: 0.00060 | Acc: 44.56% | Best: 44.56% | Time: 2.6m
Ep 012 | Loss: 3.4086 | LR: 0.00065 | Acc: 46.36% | Best: 46.36% | Time: 2.8m
Ep 013 | Loss: 3.3310 | LR: 0.00070 | Acc: 46.76% | Best: 46.76% | Time: 3.0m
Ep 014 | Loss: 3.3689 | LR: 0.00075 | Acc: 47.52% | Best: 47.52% | Time: 3.3m
Ep 015 | Loss: 3.3082 | LR: 0.00080 | Acc: 49.20% | Best: 49.20% | Time: 3.5m
Ep 016 | Loss: 3.2149 | LR: 0.00085 | Acc: 50.32% | Best: 50.32% | Time: 3.7m
Ep 017 | Loss: 3.1905 | LR: 0.00090 | Acc: 50.20% | Best: 50.32% | Time: 3.9m
Ep 018 | Loss: 3.2080 | LR: 0.00095 | Acc: 49.60% | Best: 50.32% | Time: 4.2m
Ep 019 | Loss: 3.1619 | LR: 0.00100 | Acc: 48.96% | Best: 50.32% | Time: 4.4m
Ep 020 | Loss: 3.2176 | LR: 0.00100 | Acc: 51.92% | Best: 51.92% | Time: 4.6m
Ep 021 | Loss: 3.1690 | LR: 0.00100 | Acc: 51.48% | Best: 51.92% | Time: 4.9m
Ep 022 | Loss: 3.1628 | LR: 0.00100 | Acc: 53.68% | Best: 53.68% | Time: 5.1m
Ep 023 | Loss: 3.0997 | LR: 0.00100 | Acc: 54.36% | Best: 54.36% | Time: 5.3m
Ep 024 | Loss: 2.9980 | LR: 0.00100 | Acc: 57.56% | Best: 57.56% | Time: 5.6m
Ep 025 | Loss: 3.0708 | LR: 0.00100 | Acc: 55.84% | Best: 57.56% | Time: 5.8m
Ep 026 | Loss: 3.0057 | LR: 0.00100 | Acc: 58.60% | Best: 58.60% | Time: 6.0m
Ep 027 | Loss: 2.9668 | LR: 0.00100 | Acc: 59.08% | Best: 59.08% | Time: 6.2m
Ep 028 | Loss: 2.9158 | LR: 0.00100 | Acc: 59.76% | Best: 59.76% | Time: 6.5m
Ep 029 | Loss: 3.0053 | LR: 0.00100 | Acc: 60.48% | Best: 60.48% | Time: 6.7m
Ep 030 | Loss: 2.9528 | LR: 0.00100 | Acc: 61.08% | Best: 61.08% | Time: 6.9m
Ep 031 | Loss: 2.9321 | LR: 0.00100 | Acc: 59.32% | Best: 61.08% | Time: 7.2m
Ep 032 | Loss: 2.9285 | LR: 0.00100 | Acc: 61.48% | Best: 61.48% | Time: 7.4m
Ep 033 | Loss: 2.8044 | LR: 0.00099 | Acc: 63.28% | Best: 63.28% | Time: 7.7m
Ep 034 | Loss: 2.8841 | LR: 0.00099 | Acc: 61.52% | Best: 63.28% | Time: 7.9m
Ep 035 | Loss: 2.7801 | LR: 0.00099 | Acc: 65.40% | Best: 65.40% | Time: 8.1m
Ep 036 | Loss: 2.6800 | LR: 0.00099 | Acc: 64.64% | Best: 65.40% | Time: 8.4m
Ep 037 | Loss: 2.6630 | LR: 0.00099 | Acc: 63.96% | Best: 65.40% | Time: 8.6m
Ep 038 | Loss: 2.6608 | LR: 0.00099 | Acc: 64.56% | Best: 65.40% | Time: 8.8m
Ep 039 | Loss: 2.6405 | LR: 0.00099 | Acc: 65.24% | Best: 65.40% | Time: 9.1m
Ep 040 | Loss: 2.6595 | LR: 0.00099 | Acc: 64.88% | Best: 65.40% | Time: 9.3m
Ep 041 | Loss: 2.6707 | LR: 0.00099 | Acc: 64.32% | Best: 65.40% | Time: 9.5m
Ep 042 | Loss: 2.6578 | LR: 0.00098 | Acc: 64.92% | Best: 65.40% | Time: 9.8m
Ep 043 | Loss: 2.6106 | LR: 0.00098 | Acc: 66.56% | Best: 66.56% | Time: 10.0m
Ep 044 | Loss: 2.7677 | LR: 0.00098 | Acc: 67.00% | Best: 67.00% | Time: 10.3m
Ep 045 | Loss: 2.6188 | LR: 0.00098 | Acc: 65.72% | Best: 67.00% | Time: 10.5m
Ep 046 | Loss: 2.4924 | LR: 0.00098 | Acc: 66.72% | Best: 67.00% | Time: 10.7m
Ep 047 | Loss: 2.5202 | LR: 0.00098 | Acc: 68.08% | Best: 68.08% | Time: 11.0m
Ep 048 | Loss: 2.5343 | LR: 0.00098 | Acc: 67.56% | Best: 68.08% | Time: 11.2m
Ep 049 | Loss: 2.5196 | LR: 0.00097 | Acc: 68.76% | Best: 68.76% | Time: 11.4m
Ep 050 | Loss: 2.4369 | LR: 0.00097 | Acc: 67.48% | Best: 68.76% | Time: 11.7m
Ep 051 | Loss: 2.4705 | LR: 0.00097 | Acc: 67.92% | Best: 68.76% | Time: 11.9m
Ep 052 | Loss: 2.5793 | LR: 0.00097 | Acc: 68.76% | Best: 68.76% | Time: 12.1m
Ep 053 | Loss: 2.6610 | LR: 0.00097 | Acc: 69.60% | Best: 69.60% | Time: 12.4m
Ep 054 | Loss: 2.3547 | LR: 0.00096 | Acc: 68.12% | Best: 69.60% | Time: 12.6m
Ep 055 | Loss: 2.4471 | LR: 0.00096 | Acc: 69.68% | Best: 69.68% | Time: 12.9m
Ep 056 | Loss: 2.3810 | LR: 0.00096 | Acc: 68.04% | Best: 69.68% | Time: 13.1m
Ep 057 | Loss: 2.4874 | LR: 0.00096 | Acc: 69.20% | Best: 69.68% | Time: 13.3m
Ep 058 | Loss: 2.4821 | LR: 0.00096 | Acc: 69.92% | Best: 69.92% | Time: 13.6m
Ep 059 | Loss: 2.4322 | LR: 0.00095 | Acc: 70.36% | Best: 70.36% | Time: 13.8m
Ep 060 | Loss: 2.4105 | LR: 0.00095 | Acc: 69.80% | Best: 70.36% | Time: 14.1m
Ep 061 | Loss: 2.3807 | LR: 0.00095 | Acc: 69.52% | Best: 70.36% | Time: 14.3m
Ep 062 | Loss: 2.4915 | LR: 0.00095 | Acc: 71.32% | Best: 71.32% | Time: 14.5m
Ep 063 | Loss: 2.3543 | LR: 0.00094 | Acc: 71.32% | Best: 71.32% | Time: 14.8m
Ep 064 | Loss: 2.2640 | LR: 0.00094 | Acc: 71.40% | Best: 71.40% | Time: 15.0m
Ep 065 | Loss: 2.3708 | LR: 0.00094 | Acc: 71.56% | Best: 71.56% | Time: 15.3m
Ep 066 | Loss: 2.3962 | LR: 0.00093 | Acc: 70.12% | Best: 71.56% | Time: 15.5m
Ep 067 | Loss: 2.3936 | LR: 0.00093 | Acc: 70.56% | Best: 71.56% | Time: 15.7m
Ep 068 | Loss: 2.4723 | LR: 0.00093 | Acc: 70.52% | Best: 71.56% | Time: 16.0m
Ep 069 | Loss: 2.2525 | LR: 0.00093 | Acc: 70.92% | Best: 71.56% | Time: 16.2m
Ep 070 | Loss: 2.2356 | LR: 0.00092 | Acc: 72.68% | Best: 72.68% | Time: 16.4m
Ep 071 | Loss: 2.4146 | LR: 0.00092 | Acc: 71.68% | Best: 72.68% | Time: 16.7m
Ep 072 | Loss: 2.3217 | LR: 0.00092 | Acc: 71.64% | Best: 72.68% | Time: 16.9m
Ep 073 | Loss: 2.0480 | LR: 0.00091 | Acc: 71.96% | Best: 72.68% | Time: 17.1m
Ep 074 | Loss: 2.1839 | LR: 0.00091 | Acc: 72.48% | Best: 72.68% | Time: 17.4m
Ep 075 | Loss: 2.3441 | LR: 0.00091 | Acc: 71.52% | Best: 72.68% | Time: 17.6m
Ep 076 | Loss: 2.0946 | LR: 0.00090 | Acc: 72.80% | Best: 72.80% | Time: 17.9m
Ep 077 | Loss: 2.2630 | LR: 0.00090 | Acc: 72.28% | Best: 72.80% | Time: 18.1m
Ep 078 | Loss: 2.3100 | LR: 0.00090 | Acc: 73.76% | Best: 73.76% | Time: 18.3m
Ep 079 | Loss: 2.2794 | LR: 0.00089 | Acc: 72.20% | Best: 73.76% | Time: 18.6m
Ep 080 | Loss: 2.1361 | LR: 0.00089 | Acc: 71.96% | Best: 73.76% | Time: 18.8m
Ep 081 | Loss: 2.2083 | LR: 0.00089 | Acc: 72.80% | Best: 73.76% | Time: 19.0m
Ep 082 | Loss: 2.0415 | LR: 0.00088 | Acc: 73.16% | Best: 73.76% | Time: 19.3m
Ep 083 | Loss: 2.2795 | LR: 0.00088 | Acc: 72.68% | Best: 73.76% | Time: 19.5m
Ep 084 | Loss: 2.1693 | LR: 0.00088 | Acc: 73.04% | Best: 73.76% | Time: 19.7m
Ep 085 | Loss: 2.2413 | LR: 0.00087 | Acc: 72.96% | Best: 73.76% | Time: 20.0m
Ep 086 | Loss: 2.2313 | LR: 0.00087 | Acc: 73.68% | Best: 73.76% | Time: 20.2m
Ep 087 | Loss: 2.2555 | LR: 0.00087 | Acc: 74.12% | Best: 74.12% | Time: 20.5m
Ep 088 | Loss: 2.2372 | LR: 0.00086 | Acc: 73.24% | Best: 74.12% | Time: 20.7m
Ep 089 | Loss: 2.1303 | LR: 0.00086 | Acc: 73.92% | Best: 74.12% | Time: 20.9m
Ep 090 | Loss: 2.1653 | LR: 0.00085 | Acc: 73.24% | Best: 74.12% | Time: 21.2m
Ep 091 | Loss: 2.1393 | LR: 0.00085 | Acc: 74.12% | Best: 74.12% | Time: 21.4m
Ep 092 | Loss: 2.1670 | LR: 0.00085 | Acc: 74.96% | Best: 74.96% | Time: 21.6m
Ep 093 | Loss: 2.0931 | LR: 0.00084 | Acc: 74.44% | Best: 74.96% | Time: 21.9m
Ep 094 | Loss: 2.1476 | LR: 0.00084 | Acc: 73.76% | Best: 74.96% | Time: 22.1m
Ep 095 | Loss: 2.2989 | LR: 0.00083 | Acc: 73.76% | Best: 74.96% | Time: 22.4m
Ep 096 | Loss: 2.0402 | LR: 0.00083 | Acc: 74.24% | Best: 74.96% | Time: 22.6m
Ep 097 | Loss: 2.0992 | LR: 0.00082 | Acc: 74.40% | Best: 74.96% | Time: 22.8m
Ep 098 | Loss: 2.1082 | LR: 0.00082 | Acc: 73.92% | Best: 74.96% | Time: 23.1m
Ep 099 | Loss: 2.2377 | LR: 0.00082 | Acc: 74.32% | Best: 74.96% | Time: 23.3m
Ep 100 | Loss: 2.1224 | LR: 0.00081 | Acc: 73.56% | Best: 74.96% | Time: 23.5m
Ep 101 | Loss: 2.0675 | LR: 0.00081 | Acc: 74.00% | Best: 74.96% | Time: 23.8m
Ep 102 | Loss: 2.1726 | LR: 0.00080 | Acc: 74.84% | Best: 74.96% | Time: 24.0m
Ep 103 | Loss: 2.1632 | LR: 0.00080 | Acc: 75.16% | Best: 75.16% | Time: 24.2m
Ep 104 | Loss: 2.0133 | LR: 0.00079 | Acc: 73.72% | Best: 75.16% | Time: 24.5m
Ep 105 | Loss: 2.1147 | LR: 0.00079 | Acc: 74.60% | Best: 75.16% | Time: 24.7m
Ep 106 | Loss: 2.0794 | LR: 0.00078 | Acc: 74.48% | Best: 75.16% | Time: 25.0m
Ep 107 | Loss: 1.9966 | LR: 0.00078 | Acc: 74.60% | Best: 75.16% | Time: 25.2m
Ep 108 | Loss: 2.1957 | LR: 0.00078 | Acc: 74.76% | Best: 75.16% | Time: 25.4m
Ep 109 | Loss: 2.0436 | LR: 0.00077 | Acc: 74.60% | Best: 75.16% | Time: 25.7m
Ep 110 | Loss: 2.0890 | LR: 0.00077 | Acc: 75.12% | Best: 75.16% | Time: 25.9m
Ep 111 | Loss: 2.1088 | LR: 0.00076 | Acc: 76.32% | Best: 76.32% | Time: 26.1m
Ep 112 | Loss: 2.0633 | LR: 0.00076 | Acc: 75.24% | Best: 76.32% | Time: 26.4m
Ep 113 | Loss: 2.0537 | LR: 0.00075 | Acc: 75.48% | Best: 76.32% | Time: 26.6m
Ep 114 | Loss: 2.0287 | LR: 0.00075 | Acc: 75.84% | Best: 76.32% | Time: 26.8m
Ep 115 | Loss: 2.1067 | LR: 0.00074 | Acc: 74.68% | Best: 76.32% | Time: 27.1m
Ep 116 | Loss: 1.9636 | LR: 0.00074 | Acc: 74.16% | Best: 76.32% | Time: 27.3m
Ep 117 | Loss: 2.0103 | LR: 0.00073 | Acc: 75.92% | Best: 76.32% | Time: 27.5m
Ep 118 | Loss: 2.1161 | LR: 0.00073 | Acc: 75.88% | Best: 76.32% | Time: 27.8m
Ep 119 | Loss: 1.9706 | LR: 0.00072 | Acc: 75.20% | Best: 76.32% | Time: 28.0m
Ep 120 | Loss: 1.9397 | LR: 0.00072 | Acc: 75.00% | Best: 76.32% | Time: 28.3m
Ep 121 | Loss: 2.1104 | LR: 0.00071 | Acc: 75.08% | Best: 76.32% | Time: 28.5m
Ep 122 | Loss: 2.0369 | LR: 0.00071 | Acc: 75.92% | Best: 76.32% | Time: 28.7m
Ep 123 | Loss: 1.9451 | LR: 0.00070 | Acc: 75.48% | Best: 76.32% | Time: 29.0m
Ep 124 | Loss: 2.1488 | LR: 0.00070 | Acc: 76.24% | Best: 76.32% | Time: 29.2m
Ep 125 | Loss: 1.9252 | LR: 0.00069 | Acc: 75.80% | Best: 76.32% | Time: 29.4m
Ep 126 | Loss: 2.0782 | LR: 0.00069 | Acc: 75.80% | Best: 76.32% | Time: 29.7m
Ep 127 | Loss: 1.9356 | LR: 0.00068 | Acc: 76.04% | Best: 76.32% | Time: 29.9m
Ep 128 | Loss: 2.0587 | LR: 0.00068 | Acc: 76.08% | Best: 76.32% | Time: 30.1m
Ep 129 | Loss: 1.9984 | LR: 0.00067 | Acc: 76.96% | Best: 76.96% | Time: 30.4m
Ep 130 | Loss: 1.9199 | LR: 0.00067 | Acc: 76.84% | Best: 76.96% | Time: 30.6m
Ep 131 | Loss: 1.9883 | LR: 0.00066 | Acc: 76.20% | Best: 76.96% | Time: 30.8m
Ep 132 | Loss: 1.9636 | LR: 0.00065 | Acc: 76.72% | Best: 76.96% | Time: 31.1m
Ep 133 | Loss: 2.1139 | LR: 0.00065 | Acc: 75.56% | Best: 76.96% | Time: 31.3m
Ep 134 | Loss: 1.9161 | LR: 0.00064 | Acc: 75.52% | Best: 76.96% | Time: 31.6m
Ep 135 | Loss: 2.0110 | LR: 0.00064 | Acc: 76.52% | Best: 76.96% | Time: 31.8m
Ep 136 | Loss: 2.0452 | LR: 0.00063 | Acc: 75.24% | Best: 76.96% | Time: 32.0m
Ep 137 | Loss: 1.9869 | LR: 0.00063 | Acc: 76.80% | Best: 76.96% | Time: 32.3m
Ep 138 | Loss: 1.9209 | LR: 0.00062 | Acc: 76.80% | Best: 76.96% | Time: 32.5m
Ep 139 | Loss: 1.9712 | LR: 0.00062 | Acc: 76.64% | Best: 76.96% | Time: 32.7m
Ep 140 | Loss: 1.8445 | LR: 0.00061 | Acc: 76.36% | Best: 76.96% | Time: 33.0m
Ep 141 | Loss: 1.9214 | LR: 0.00061 | Acc: 76.76% | Best: 76.96% | Time: 33.2m
Ep 142 | Loss: 1.9938 | LR: 0.00060 | Acc: 77.04% | Best: 77.04% | Time: 33.5m
Ep 143 | Loss: 1.9921 | LR: 0.00059 | Acc: 76.44% | Best: 77.04% | Time: 33.7m
Ep 144 | Loss: 1.9594 | LR: 0.00059 | Acc: 76.68% | Best: 77.04% | Time: 33.9m
Ep 145 | Loss: 1.9693 | LR: 0.00058 | Acc: 76.20% | Best: 77.04% | Time: 34.2m
Ep 146 | Loss: 1.8905 | LR: 0.00058 | Acc: 77.08% | Best: 77.08% | Time: 34.4m
Ep 147 | Loss: 1.8028 | LR: 0.00057 | Acc: 76.80% | Best: 77.08% | Time: 34.6m
Ep 148 | Loss: 1.8455 | LR: 0.00057 | Acc: 76.84% | Best: 77.08% | Time: 34.9m
Ep 149 | Loss: 1.9731 | LR: 0.00056 | Acc: 76.52% | Best: 77.08% | Time: 35.1m
Ep 150 | Loss: 1.8800 | LR: 0.00056 | Acc: 77.00% | Best: 77.08% | Time: 35.3m
Ep 151 | Loss: 2.0477 | LR: 0.00055 | Acc: 77.84% | Best: 77.84% | Time: 35.6m
Ep 152 | Loss: 1.9231 | LR: 0.00054 | Acc: 77.64% | Best: 77.84% | Time: 35.8m
Ep 153 | Loss: 2.0511 | LR: 0.00054 | Acc: 76.88% | Best: 77.84% | Time: 36.0m
Ep 154 | Loss: 1.8730 | LR: 0.00053 | Acc: 76.48% | Best: 77.84% | Time: 36.3m
Ep 155 | Loss: 1.8615 | LR: 0.00053 | Acc: 77.60% | Best: 77.84% | Time: 36.5m
Ep 156 | Loss: 1.8335 | LR: 0.00052 | Acc: 77.44% | Best: 77.84% | Time: 36.8m
Ep 157 | Loss: 1.8235 | LR: 0.00052 | Acc: 77.88% | Best: 77.88% | Time: 37.0m
Ep 158 | Loss: 1.8850 | LR: 0.00051 | Acc: 77.88% | Best: 77.88% | Time: 37.2m
Ep 159 | Loss: 1.9894 | LR: 0.00051 | Acc: 78.00% | Best: 78.00% | Time: 37.5m
Ep 160 | Loss: 1.8420 | LR: 0.00050 | Acc: 77.56% | Best: 78.00% | Time: 37.7m
Ep 161 | Loss: 1.8632 | LR: 0.00049 | Acc: 77.88% | Best: 78.00% | Time: 37.9m
Ep 162 | Loss: 1.7946 | LR: 0.00049 | Acc: 77.56% | Best: 78.00% | Time: 38.2m
Ep 163 | Loss: 2.0024 | LR: 0.00048 | Acc: 77.88% | Best: 78.00% | Time: 38.4m
Ep 164 | Loss: 1.8074 | LR: 0.00048 | Acc: 77.64% | Best: 78.00% | Time: 38.7m
Ep 165 | Loss: 1.9413 | LR: 0.00047 | Acc: 77.56% | Best: 78.00% | Time: 38.9m
Ep 166 | Loss: 1.8686 | LR: 0.00047 | Acc: 77.56% | Best: 78.00% | Time: 39.1m
Ep 167 | Loss: 1.8305 | LR: 0.00046 | Acc: 78.16% | Best: 78.16% | Time: 39.4m
Ep 168 | Loss: 1.8201 | LR: 0.00046 | Acc: 78.12% | Best: 78.16% | Time: 39.6m
Ep 169 | Loss: 1.8005 | LR: 0.00045 | Acc: 78.24% | Best: 78.24% | Time: 39.8m
Ep 170 | Loss: 1.8777 | LR: 0.00044 | Acc: 77.72% | Best: 78.24% | Time: 40.1m
Ep 171 | Loss: 1.8224 | LR: 0.00044 | Acc: 77.88% | Best: 78.24% | Time: 40.3m
Ep 172 | Loss: 1.7318 | LR: 0.00043 | Acc: 78.84% | Best: 78.84% | Time: 40.6m
Ep 173 | Loss: 1.8646 | LR: 0.00043 | Acc: 77.92% | Best: 78.84% | Time: 40.8m
Ep 174 | Loss: 1.9610 | LR: 0.00042 | Acc: 78.80% | Best: 78.84% | Time: 41.0m
Ep 175 | Loss: 1.8125 | LR: 0.00042 | Acc: 79.32% | Best: 79.32% | Time: 41.3m
Ep 176 | Loss: 1.9058 | LR: 0.00041 | Acc: 77.88% | Best: 79.32% | Time: 41.5m
Ep 177 | Loss: 1.8965 | LR: 0.00041 | Acc: 78.36% | Best: 79.32% | Time: 41.8m
Ep 178 | Loss: 1.9567 | LR: 0.00040 | Acc: 78.48% | Best: 79.32% | Time: 42.0m
Ep 179 | Loss: 1.7123 | LR: 0.00039 | Acc: 78.28% | Best: 79.32% | Time: 42.2m
Ep 180 | Loss: 1.8256 | LR: 0.00039 | Acc: 78.96% | Best: 79.32% | Time: 42.5m
Ep 181 | Loss: 1.7436 | LR: 0.00038 | Acc: 77.84% | Best: 79.32% | Time: 42.7m
Ep 182 | Loss: 1.6899 | LR: 0.00038 | Acc: 78.48% | Best: 79.32% | Time: 43.0m
Ep 183 | Loss: 1.8764 | LR: 0.00037 | Acc: 78.64% | Best: 79.32% | Time: 43.2m
Ep 184 | Loss: 1.7645 | LR: 0.00037 | Acc: 79.00% | Best: 79.32% | Time: 43.4m
Ep 185 | Loss: 1.9520 | LR: 0.00036 | Acc: 78.72% | Best: 79.32% | Time: 43.7m
Ep 186 | Loss: 1.8218 | LR: 0.00036 | Acc: 78.08% | Best: 79.32% | Time: 43.9m
Ep 187 | Loss: 1.7473 | LR: 0.00035 | Acc: 79.24% | Best: 79.32% | Time: 44.2m
Ep 188 | Loss: 1.9126 | LR: 0.00035 | Acc: 79.60% | Best: 79.60% | Time: 44.4m
Ep 189 | Loss: 1.8011 | LR: 0.00034 | Acc: 79.32% | Best: 79.60% | Time: 44.6m
Ep 190 | Loss: 1.8085 | LR: 0.00033 | Acc: 78.76% | Best: 79.60% | Time: 44.9m
Ep 191 | Loss: 1.7421 | LR: 0.00033 | Acc: 79.12% | Best: 79.60% | Time: 45.1m
Ep 192 | Loss: 1.7318 | LR: 0.00032 | Acc: 79.40% | Best: 79.60% | Time: 45.4m
Ep 193 | Loss: 1.9276 | LR: 0.00032 | Acc: 78.32% | Best: 79.60% | Time: 45.6m
Ep 194 | Loss: 1.8850 | LR: 0.00031 | Acc: 79.52% | Best: 79.60% | Time: 45.8m
Ep 195 | Loss: 1.7778 | LR: 0.00031 | Acc: 79.28% | Best: 79.60% | Time: 46.1m
Ep 196 | Loss: 1.5904 | LR: 0.00030 | Acc: 79.48% | Best: 79.60% | Time: 46.3m
Ep 197 | Loss: 1.7477 | LR: 0.00030 | Acc: 78.84% | Best: 79.60% | Time: 46.6m
Ep 198 | Loss: 1.6886 | LR: 0.00029 | Acc: 79.32% | Best: 79.60% | Time: 46.8m
Ep 199 | Loss: 1.8660 | LR: 0.00029 | Acc: 78.84% | Best: 79.60% | Time: 47.0m
Ep 200 | Loss: 1.7497 | LR: 0.00028 | Acc: 79.72% | Best: 79.72% | Time: 47.3m
Ep 201 | Loss: 1.8247 | LR: 0.00028 | Acc: 78.52% | Best: 79.72% | Time: 47.5m
Ep 202 | Loss: 1.8078 | LR: 0.00027 | Acc: 78.56% | Best: 79.72% | Time: 47.8m
Ep 203 | Loss: 1.7465 | LR: 0.00027 | Acc: 79.40% | Best: 79.72% | Time: 48.0m
Ep 204 | Loss: 1.7561 | LR: 0.00026 | Acc: 79.40% | Best: 79.72% | Time: 48.3m
Ep 205 | Loss: 1.7677 | LR: 0.00026 | Acc: 79.64% | Best: 79.72% | Time: 48.5m
Ep 206 | Loss: 1.7310 | LR: 0.00025 | Acc: 80.00% | Best: 80.00% | Time: 48.7m
Ep 207 | Loss: 1.7241 | LR: 0.00025 | Acc: 80.32% | Best: 80.32% | Time: 49.0m
Ep 208 | Loss: 1.7556 | LR: 0.00024 | Acc: 78.80% | Best: 80.32% | Time: 49.2m
Ep 209 | Loss: 1.6727 | LR: 0.00024 | Acc: 80.00% | Best: 80.32% | Time: 49.5m
Ep 210 | Loss: 1.6147 | LR: 0.00023 | Acc: 79.80% | Best: 80.32% | Time: 49.7m
Ep 211 | Loss: 1.7966 | LR: 0.00023 | Acc: 80.36% | Best: 80.36% | Time: 50.0m
Ep 212 | Loss: 1.7554 | LR: 0.00022 | Acc: 80.04% | Best: 80.36% | Time: 50.2m
Ep 213 | Loss: 1.6263 | LR: 0.00022 | Acc: 80.56% | Best: 80.56% | Time: 50.4m
Ep 214 | Loss: 1.7768 | LR: 0.00022 | Acc: 79.36% | Best: 80.56% | Time: 50.7m
Ep 215 | Loss: 1.8405 | LR: 0.00021 | Acc: 80.20% | Best: 80.56% | Time: 50.9m
Ep 216 | Loss: 1.8229 | LR: 0.00021 | Acc: 79.96% | Best: 80.56% | Time: 51.2m
Ep 217 | Loss: 1.6910 | LR: 0.00020 | Acc: 79.96% | Best: 80.56% | Time: 51.4m
Ep 218 | Loss: 1.7247 | LR: 0.00020 | Acc: 80.00% | Best: 80.56% | Time: 51.7m
Ep 219 | Loss: 1.7867 | LR: 0.00019 | Acc: 79.68% | Best: 80.56% | Time: 51.9m
Ep 220 | Loss: 1.8067 | LR: 0.00019 | Acc: 79.96% | Best: 80.56% | Time: 52.1m
k3]:[W128 00:57:30.160267170 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
Ep 220 | Loss: 1.7475 | LR: 0.00019 | Acc: 79.80% | Best: 80.56% | Time: 0.3m
Ep 221 | Loss: 1.8081 | LR: 0.00018 | Acc: 79.52% | Best: 80.56% | Time: 0.6m
Ep 222 | Loss: 1.7189 | LR: 0.00018 | Acc: 80.72% | Best: 80.72% | Time: 0.8m
Ep 223 | Loss: 1.7987 | LR: 0.00018 | Acc: 80.24% | Best: 80.72% | Time: 1.1m
Ep 224 | Loss: 1.7383 | LR: 0.00017 | Acc: 79.96% | Best: 80.72% | Time: 1.3m
Ep 225 | Loss: 1.8709 | LR: 0.00017 | Acc: 79.80% | Best: 80.72% | Time: 1.5m
Ep 226 | Loss: 1.6758 | LR: 0.00016 | Acc: 80.32% | Best: 80.72% | Time: 1.8m
Ep 227 | Loss: 1.7102 | LR: 0.00016 | Acc: 80.68% | Best: 80.72% | Time: 2.0m
Ep 228 | Loss: 1.6277 | LR: 0.00015 | Acc: 80.48% | Best: 80.72% | Time: 2.3m
Ep 229 | Loss: 1.7749 | LR: 0.00015 | Acc: 80.28% | Best: 80.72% | Time: 2.5m
Ep 230 | Loss: 1.6793 | LR: 0.00015 | Acc: 80.00% | Best: 80.72% | Time: 2.8m
Ep 231 | Loss: 1.6818 | LR: 0.00014 | Acc: 80.76% | Best: 80.76% | Time: 3.0m
Ep 232 | Loss: 1.6805 | LR: 0.00014 | Acc: 80.44% | Best: 80.76% | Time: 3.3m
Ep 233 | Loss: 1.8222 | LR: 0.00013 | Acc: 80.08% | Best: 80.76% | Time: 3.5m
Ep 234 | Loss: 1.6431 | LR: 0.00013 | Acc: 80.72% | Best: 80.76% | Time: 3.8m
Ep 235 | Loss: 1.5769 | LR: 0.00013 | Acc: 81.40% | Best: 81.40% | Time: 4.0m
Ep 236 | Loss: 1.6336 | LR: 0.00012 | Acc: 81.16% | Best: 81.40% | Time: 4.3m
Ep 237 | Loss: 1.7476 | LR: 0.00012 | Acc: 80.92% | Best: 81.40% | Time: 4.5m
Ep 238 | Loss: 1.5943 | LR: 0.00012 | Acc: 80.48% | Best: 81.40% | Time: 4.8m
Ep 239 | Loss: 1.6632 | LR: 0.00011 | Acc: 81.20% | Best: 81.40% | Time: 5.0m
Ep 240 | Loss: 1.7414 | LR: 0.00011 | Acc: 81.48% | Best: 81.48% | Time: 5.3m
Ep 241 | Loss: 1.6782 | LR: 0.00011 | Acc: 81.00% | Best: 81.48% | Time: 5.5m
Ep 242 | Loss: 1.6013 | LR: 0.00010 | Acc: 81.36% | Best: 81.48% | Time: 5.7m
Ep 243 | Loss: 1.6957 | LR: 0.00010 | Acc: 81.44% | Best: 81.48% | Time: 6.0m
Ep 244 | Loss: 1.7370 | LR: 0.00010 | Acc: 81.36% | Best: 81.48% | Time: 6.2m
Ep 245 | Loss: 1.6898 | LR: 0.00009 | Acc: 81.12% | Best: 81.48% | Time: 6.5m
Ep 246 | Loss: 1.7596 | LR: 0.00009 | Acc: 81.52% | Best: 81.52% | Time: 6.7m
Ep 247 | Loss: 1.6755 | LR: 0.00009 | Acc: 81.48% | Best: 81.52% | Time: 7.0m
Ep 248 | Loss: 1.6495 | LR: 0.00008 | Acc: 81.64% | Best: 81.64% | Time: 7.2m
Ep 249 | Loss: 1.6333 | LR: 0.00008 | Acc: 80.40% | Best: 81.64% | Time: 7.5m
Ep 250 | Loss: 1.6928 | LR: 0.00008 | Acc: 80.76% | Best: 81.64% | Time: 7.7m
Ep 251 | Loss: 1.7056 | LR: 0.00007 | Acc: 80.56% | Best: 81.64% | Time: 8.0m
Ep 252 | Loss: 1.6586 | LR: 0.00007 | Acc: 81.04% | Best: 81.64% | Time: 8.2m
Ep 253 | Loss: 1.7092 | LR: 0.00007 | Acc: 81.24% | Best: 81.64% | Time: 8.5m
Ep 254 | Loss: 1.6785 | LR: 0.00007 | Acc: 81.36% | Best: 81.64% | Time: 8.7m
Ep 255 | Loss: 1.6026 | LR: 0.00006 | Acc: 81.40% | Best: 81.64% | Time: 9.0m
Ep 256 | Loss: 1.5998 | LR: 0.00006 | Acc: 81.00% | Best: 81.64% | Time: 9.2m
Ep 257 | Loss: 1.6131 | LR: 0.00006 | Acc: 81.36% | Best: 81.64% | Time: 9.5m
Ep 258 | Loss: 1.6622 | LR: 0.00005 | Acc: 80.24% | Best: 81.64% | Time: 9.7m
Ep 259 | Loss: 1.6681 | LR: 0.00005 | Acc: 81.00% | Best: 81.64% | Time: 9.9m
Ep 260 | Loss: 1.6719 | LR: 0.00005 | Acc: 81.08% | Best: 81.64% | Time: 10.2m
Ep 261 | Loss: 1.6425 | LR: 0.00005 | Acc: 81.44% | Best: 81.64% | Time: 10.4m
Ep 262 | Loss: 1.7117 | LR: 0.00004 | Acc: 81.36% | Best: 81.64% | Time: 10.7m
Ep 263 | Loss: 1.7160 | LR: 0.00004 | Acc: 81.12% | Best: 81.64% | Time: 10.9m
Ep 264 | Loss: 1.5657 | LR: 0.00004 | Acc: 80.64% | Best: 81.64% | Time: 11.2m
Ep 265 | Loss: 1.6396 | LR: 0.00004 | Acc: 81.12% | Best: 81.64% | Time: 11.4m
Ep 266 | Loss: 1.5766 | LR: 0.00004 | Acc: 81.32% | Best: 81.64% | Time: 11.7m
Ep 267 | Loss: 1.6581 | LR: 0.00003 | Acc: 81.24% | Best: 81.64% | Time: 11.9m
Ep 268 | Loss: 1.5791 | LR: 0.00003 | Acc: 81.08% | Best: 81.64% | Time: 12.2m
Ep 269 | Loss: 1.6289 | LR: 0.00003 | Acc: 81.68% | Best: 81.68% | Time: 12.4m
Ep 270 | Loss: 1.6465 | LR: 0.00003 | Acc: 81.16% | Best: 81.68% | Time: 12.7m
Ep 271 | Loss: 1.6348 | LR: 0.00003 | Acc: 80.96% | Best: 81.68% | Time: 12.9m
Ep 272 | Loss: 1.6878 | LR: 0.00002 | Acc: 81.24% | Best: 81.68% | Time: 13.2m
Ep 273 | Loss: 1.6128 | LR: 0.00002 | Acc: 81.32% | Best: 81.68% | Time: 13.4m
Ep 274 | Loss: 1.5980 | LR: 0.00002 | Acc: 81.60% | Best: 81.68% | Time: 13.7m
Ep 275 | Loss: 1.6432 | LR: 0.00002 | Acc: 81.24% | Best: 81.68% | Time: 13.9m
Ep 276 | Loss: 1.7061 | LR: 0.00002 | Acc: 81.12% | Best: 81.68% | Time: 14.1m
Ep 277 | Loss: 1.6710 | LR: 0.00002 | Acc: 81.24% | Best: 81.68% | Time: 14.4m
Ep 278 | Loss: 1.6383 | LR: 0.00002 | Acc: 81.36% | Best: 81.68% | Time: 14.6m
Ep 279 | Loss: 1.6801 | LR: 0.00001 | Acc: 81.16% | Best: 81.68% | Time: 14.9m
Ep 280 | Loss: 1.6139 | LR: 0.00001 | Acc: 81.48% | Best: 81.68% | Time: 15.1m
Ep 281 | Loss: 1.6537 | LR: 0.00001 | Acc: 80.84% | Best: 81.68% | Time: 15.4m
Ep 282 | Loss: 1.4973 | LR: 0.00001 | Acc: 81.84% | Best: 81.84% | Time: 15.6m
Ep 283 | Loss: 1.6537 | LR: 0.00001 | Acc: 81.56% | Best: 81.84% | Time: 15.9m
Ep 284 | Loss: 1.6670 | LR: 0.00001 | Acc: 81.24% | Best: 81.84% | Time: 16.1m
Ep 285 | Loss: 1.6790 | LR: 0.00001 | Acc: 81.44% | Best: 81.84% | Time: 16.4m
Ep 286 | Loss: 1.7263 | LR: 0.00001 | Acc: 81.08% | Best: 81.84% | Time: 16.6m
Ep 287 | Loss: 1.5658 | LR: 0.00001 | Acc: 81.12% | Best: 81.84% | Time: 16.9m
Ep 288 | Loss: 1.7073 | LR: 0.00000 | Acc: 81.28% | Best: 81.84% | Time: 17.1m
Ep 289 | Loss: 1.5724 | LR: 0.00000 | Acc: 81.24% | Best: 81.84% | Time: 17.4m
Ep 290 | Loss: 1.6657 | LR: 0.00000 | Acc: 81.28% | Best: 81.84% | Time: 17.6m
Ep 291 | Loss: 1.6534 | LR: 0.00000 | Acc: 81.40% | Best: 81.84% | Time: 17.9m
Ep 292 | Loss: 1.5618 | LR: 0.00000 | Acc: 81.72% | Best: 81.84% | Time: 18.1m
Ep 293 | Loss: 1.5891 | LR: 0.00000 | Acc: 81.52% | Best: 81.84% | Time: 18.3m
Ep 294 | Loss: 1.6108 | LR: 0.00000 | Acc: 81.72% | Best: 81.84% | Time: 18.6m
Ep 295 | Loss: 1.6702 | LR: 0.00000 | Acc: 81.00% | Best: 81.84% | Time: 18.8m
Ep 296 | Loss: 1.6609 | LR: 0.00000 | Acc: 81.44% | Best: 81.84% | Time: 19.1m
Ep 297 | Loss: 1.6553 | LR: 0.00000 | Acc: 81.48% | Best: 81.84% | Time: 19.3m
Ep 298 | Loss: 1.5983 | LR: 0.00000 | Acc: 81.52% | Best: 81.84% | Time: 19.6m
Ep 299 | Loss: 1.6264 | LR: 0.00000 | Acc: 81.48% | Best: 81.84% | Time: 19.8m
Ep 300 | Loss: 1.5977 | LR: 0.00000 | Acc: 81.56% | Best: 81.84% | Time: 20.1m

[S1] Ep 001 | Mix:0.20 Noise:0.050 | Loss:1.0285 | Acc:81.68% | Best:81.68% | Time:0.3m
[S1] Ep 002 | Mix:0.20 Noise:0.050 | Loss:1.2464 | Acc:81.12% | Best:81.68% | Time:0.6m
[S1] Ep 003 | Mix:0.20 Noise:0.050 | Loss:0.9995 | Acc:81.52% | Best:81.68% | Time:0.9m
[S1] Ep 004 | Mix:0.20 Noise:0.050 | Loss:1.1825 | Acc:81.76% | Best:81.76% | Time:1.2m
[S1] Ep 005 | Mix:0.20 Noise:0.050 | Loss:1.1335 | Acc:81.68% | Best:81.76% | Time:1.5m
[S1] Ep 006 | Mix:0.20 Noise:0.050 | Loss:1.1700 | Acc:81.84% | Best:81.84% | Time:1.8m
[S1] Ep 007 | Mix:0.20 Noise:0.050 | Loss:1.2299 | Acc:81.20% | Best:81.84% | Time:2.1m
[S1] Ep 008 | Mix:0.20 Noise:0.050 | Loss:1.1607 | Acc:81.48% | Best:81.84% | Time:2.4m
[S1] Ep 009 | Mix:0.20 Noise:0.050 | Loss:1.2761 | Acc:81.52% | Best:81.84% | Time:2.7m
[S1] Ep 010 | Mix:0.20 Noise:0.050 | Loss:1.1485 | Acc:81.64% | Best:81.84% | Time:3.0m
[S1] Ep 011 | Mix:0.20 Noise:0.050 | Loss:1.0932 | Acc:81.52% | Best:81.84% | Time:3.3m
[S1] Ep 012 | Mix:0.20 Noise:0.050 | Loss:1.0849 | Acc:81.84% | Best:81.84% | Time:3.6m
[S1] Ep 013 | Mix:0.20 Noise:0.050 | Loss:1.1479 | Acc:81.12% | Best:81.84% | Time:3.9m
[S1] Ep 014 | Mix:0.20 Noise:0.050 | Loss:1.1568 | Acc:81.48% | Best:81.84% | Time:4.2m
[S1] Ep 015 | Mix:0.20 Noise:0.050 | Loss:1.2552 | Acc:81.16% | Best:81.84% | Time:4.5m
[S2] Ep 016 | Mix:0.10 Noise:0.025 | Loss:0.8514 | Acc:81.60% | Best:81.84% | Time:4.8m
[S2] Ep 017 | Mix:0.10 Noise:0.025 | Loss:0.6957 | Acc:81.88% | Best:81.88% | Time:5.1m
[S2] Ep 018 | Mix:0.10 Noise:0.025 | Loss:0.6869 | Acc:81.72% | Best:81.88% | Time:5.4m
[S2] Ep 019 | Mix:0.10 Noise:0.025 | Loss:0.8902 | Acc:81.80% | Best:81.88% | Time:5.7m
[S2] Ep 020 | Mix:0.10 Noise:0.025 | Loss:0.6515 | Acc:82.08% | Best:82.08% | Time:6.0m
[S2] Ep 021 | Mix:0.10 Noise:0.025 | Loss:0.9001 | Acc:81.64% | Best:82.08% | Time:6.3m
[S2] Ep 022 | Mix:0.10 Noise:0.025 | Loss:0.7660 | Acc:81.76% | Best:82.08% | Time:6.5m
[S2] Ep 023 | Mix:0.10 Noise:0.025 | Loss:0.7801 | Acc:81.56% | Best:82.08% | Time:6.8m
[S2] Ep 024 | Mix:0.10 Noise:0.025 | Loss:0.7477 | Acc:81.28% | Best:82.08% | Time:7.1m
[S2] Ep 025 | Mix:0.10 Noise:0.025 | Loss:0.8070 | Acc:81.72% | Best:82.08% | Time:7.4m
[S2] Ep 026 | Mix:0.10 Noise:0.025 | Loss:0.7632 | Acc:81.72% | Best:82.08% | Time:7.7m
[S2] Ep 027 | Mix:0.10 Noise:0.025 | Loss:0.8407 | Acc:81.72% | Best:82.08% | Time:8.0m
[S2] Ep 028 | Mix:0.10 Noise:0.025 | Loss:0.8198 | Acc:81.80% | Best:82.08% | Time:8.3m
[S2] Ep 029 | Mix:0.10 Noise:0.025 | Loss:0.8371 | Acc:82.04% | Best:82.08% | Time:8.6m
[S2] Ep 030 | Mix:0.10 Noise:0.025 | Loss:0.7744 | Acc:82.08% | Best:82.08% | Time:8.9m
[S2] Ep 031 | Mix:0.10 Noise:0.025 | Loss:0.6703 | Acc:81.96% | Best:82.08% | Time:9.2m
[S2] Ep 032 | Mix:0.10 Noise:0.025 | Loss:0.8747 | Acc:81.72% | Best:82.08% | Time:9.5m
[S2] Ep 033 | Mix:0.10 Noise:0.025 | Loss:0.7545 | Acc:81.64% | Best:82.08% | Time:9.8m
[S2] Ep 034 | Mix:0.10 Noise:0.025 | Loss:0.8486 | Acc:81.52% | Best:82.08% | Time:10.1m
[S2] Ep 035 | Mix:0.10 Noise:0.025 | Loss:0.8236 | Acc:81.76% | Best:82.08% | Time:10.3m
[S2] Ep 036 | Mix:0.10 Noise:0.025 | Loss:0.8662 | Acc:81.92% | Best:82.08% | Time:10.6m
[S2] Ep 037 | Mix:0.10 Noise:0.025 | Loss:0.7687 | Acc:81.84% | Best:82.08% | Time:10.9m
[S2] Ep 038 | Mix:0.10 Noise:0.025 | Loss:0.8047 | Acc:81.40% | Best:82.08% | Time:11.2m
[S2] Ep 039 | Mix:0.10 Noise:0.025 | Loss:0.7214 | Acc:81.92% | Best:82.08% | Time:11.5m
[S2] Ep 040 | Mix:0.10 Noise:0.025 | Loss:0.8299 | Acc:81.44% | Best:82.08% | Time:11.8m
[S2] Ep 041 | Mix:0.10 Noise:0.025 | Loss:0.7820 | Acc:81.56% | Best:82.08% | Time:12.1m
[S2] Ep 042 | Mix:0.10 Noise:0.025 | Loss:0.8195 | Acc:81.72% | Best:82.08% | Time:12.4m
[S2] Ep 043 | Mix:0.10 Noise:0.025 | Loss:0.7354 | Acc:81.88% | Best:82.08% | Time:12.7m
[S2] Ep 044 | Mix:0.10 Noise:0.025 | Loss:0.7348 | Acc:82.00% | Best:82.08% | Time:13.0m
[S2] Ep 045 | Mix:0.10 Noise:0.025 | Loss:0.8477 | Acc:81.44% | Best:82.08% | Time:13.3m
[S2] Ep 046 | Mix:0.10 Noise:0.025 | Loss:0.8107 | Acc:81.56% | Best:82.08% | Time:13.6m
[S2] Ep 047 | Mix:0.10 Noise:0.025 | Loss:0.8746 | Acc:81.56% | Best:82.08% | Time:13.9m
[S2] Ep 048 | Mix:0.10 Noise:0.025 | Loss:0.7016 | Acc:81.64% | Best:82.08% | Time:14.2m
[S2] Ep 049 | Mix:0.10 Noise:0.025 | Loss:0.8246 | Acc:81.76% | Best:82.08% | Time:14.5m
[S2] Ep 050 | Mix:0.10 Noise:0.025 | Loss:0.7504 | Acc:81.12% | Best:82.08% | Time:14.8m
[S2] Ep 051 | Mix:0.10 Noise:0.025 | Loss:0.6781 | Acc:81.56% | Best:82.08% | Time:15.0m
[S2] Ep 052 | Mix:0.10 Noise:0.025 | Loss:0.7679 | Acc:81.64% | Best:82.08% | Time:15.3m
[S2] Ep 053 | Mix:0.10 Noise:0.025 | Loss:0.8042 | Acc:81.48% | Best:82.08% | Time:15.6m
[S2] Ep 054 | Mix:0.10 Noise:0.025 | Loss:0.7266 | Acc:81.68% | Best:82.08% | Time:15.9m
[S2] Ep 055 | Mix:0.10 Noise:0.025 | Loss:0.6696 | Acc:81.92% | Best:82.08% | Time:16.2m
[S2] Ep 056 | Mix:0.10 Noise:0.025 | Loss:0.7550 | Acc:81.56% | Best:82.08% | Time:16.5m
[S2] Ep 057 | Mix:0.10 Noise:0.025 | Loss:0.8512 | Acc:82.00% | Best:82.08% | Time:16.8m
[S2] Ep 058 | Mix:0.10 Noise:0.025 | Loss:0.7421 | Acc:81.84% | Best:82.08% | Time:17.1m
[S2] Ep 059 | Mix:0.10 Noise:0.025 | Loss:0.8228 | Acc:81.96% | Best:82.08% | Time:17.4m
[S2] Ep 060 | Mix:0.10 Noise:0.025 | Loss:0.8240 | Acc:82.04% | Best:82.08% | Time:17.7m
[S2] Ep 061 | Mix:0.10 Noise:0.025 | Loss:0.7786 | Acc:82.12% | Best:82.12% | Time:18.0m
[S2] Ep 062 | Mix:0.10 Noise:0.025 | Loss:0.6851 | Acc:81.80% | Best:82.12% | Time:18.3m
[S2] Ep 063 | Mix:0.10 Noise:0.025 | Loss:0.7062 | Acc:81.76% | Best:82.12% | Time:18.5m
[S2] Ep 064 | Mix:0.10 Noise:0.025 | Loss:0.8355 | Acc:81.52% | Best:82.12% | Time:18.8m
[S2] Ep 065 | Mix:0.10 Noise:0.025 | Loss:0.6319 | Acc:81.96% | Best:82.12% | Time:19.1m
[S3] Ep 066 | Mix:0.05 Noise:0.012 | Loss:0.5903 | Acc:81.88% | Best:82.12% | Time:19.4m
[S3] Ep 067 | Mix:0.05 Noise:0.012 | Loss:0.7645 | Acc:82.00% | Best:82.12% | Time:19.7m
[S3] Ep 068 | Mix:0.05 Noise:0.012 | Loss:0.6445 | Acc:81.92% | Best:82.12% | Time:20.0m
[S3] Ep 069 | Mix:0.05 Noise:0.012 | Loss:0.6050 | Acc:82.12% | Best:82.12% | Time:20.3m
[S3] Ep 070 | Mix:0.05 Noise:0.012 | Loss:0.6855 | Acc:81.64% | Best:82.12% | Time:20.6m
[S3] Ep 071 | Mix:0.05 Noise:0.012 | Loss:0.6430 | Acc:81.96% | Best:82.12% | Time:20.8m
[S3] Ep 072 | Mix:0.05 Noise:0.012 | Loss:0.6236 | Acc:82.12% | Best:82.12% | Time:21.1m
[S3] Ep 073 | Mix:0.05 Noise:0.012 | Loss:0.5980 | Acc:82.04% | Best:82.12% | Time:21.4m
[S3] Ep 074 | Mix:0.05 Noise:0.012 | Loss:0.5928 | Acc:81.92% | Best:82.12% | Time:21.7m
[S3] Ep 075 | Mix:0.05 Noise:0.012 | Loss:0.6979 | Acc:81.92% | Best:82.12% | Time:22.0m
[S3] Ep 076 | Mix:0.05 Noise:0.012 | Loss:0.6683 | Acc:82.28% | Best:82.28% | Time:22.3m
[S3] Ep 077 | Mix:0.05 Noise:0.012 | Loss:0.6364 | Acc:82.32% | Best:82.32% | Time:22.6m
[S3] Ep 078 | Mix:0.05 Noise:0.012 | Loss:0.5144 | Acc:81.92% | Best:82.32% | Time:22.9m
[S3] Ep 079 | Mix:0.05 Noise:0.012 | Loss:0.6524 | Acc:82.28% | Best:82.32% | Time:23.2m
[S3] Ep 080 | Mix:0.05 Noise:0.012 | Loss:0.5303 | Acc:82.28% | Best:82.32% | Time:23.5m
[S3] Ep 081 | Mix:0.05 Noise:0.012 | Loss:0.6531 | Acc:81.96% | Best:82.32% | Time:23.8m
[S3] Ep 082 | Mix:0.05 Noise:0.012 | Loss:0.5423 | Acc:82.24% | Best:82.32% | Time:24.1m
[S3] Ep 083 | Mix:0.05 Noise:0.012 | Loss:0.5514 | Acc:82.04% | Best:82.32% | Time:24.4m
[S3] Ep 084 | Mix:0.05 Noise:0.012 | Loss:0.5856 | Acc:81.96% | Best:82.32% | Time:24.7m
[S3] Ep 085 | Mix:0.05 Noise:0.012 | Loss:0.5072 | Acc:82.04% | Best:82.32% | Time:24.9m
[S3] Ep 086 | Mix:0.05 Noise:0.012 | Loss:0.5034 | Acc:82.08% | Best:82.32% | Time:25.2m
[S3] Ep 087 | Mix:0.05 Noise:0.012 | Loss:0.6172 | Acc:82.20% | Best:82.32% | Time:25.5m
[S3] Ep 088 | Mix:0.05 Noise:0.012 | Loss:0.5153 | Acc:81.80% | Best:82.32% | Time:25.8m
[S3] Ep 089 | Mix:0.05 Noise:0.012 | Loss:0.6187 | Acc:82.04% | Best:82.32% | Time:26.1m
[S3] Ep 090 | Mix:0.05 Noise:0.012 | Loss:0.6083 | Acc:81.64% | Best:82.32% | Time:26.4m
[S3] Ep 091 | Mix:0.05 Noise:0.012 | Loss:0.5278 | Acc:81.88% | Best:82.32% | Time:26.7m
[S3] Ep 092 | Mix:0.05 Noise:0.012 | Loss:0.5147 | Acc:82.28% | Best:82.32% | Time:27.0m
[S3] Ep 093 | Mix:0.05 Noise:0.012 | Loss:0.6321 | Acc:81.88% | Best:82.32% | Time:27.3m
[S3] Ep 094 | Mix:0.05 Noise:0.012 | Loss:0.6454 | Acc:82.28% | Best:82.32% | Time:27.6m
[S3] Ep 095 | Mix:0.05 Noise:0.012 | Loss:0.6414 | Acc:81.92% | Best:82.32% | Time:27.9m
[S3] Ep 096 | Mix:0.05 Noise:0.012 | Loss:0.6717 | Acc:81.84% | Best:82.32% | Time:28.2m
[S3] Ep 097 | Mix:0.05 Noise:0.012 | Loss:0.6068 | Acc:82.00% | Best:82.32% | Time:28.5m
[S3] Ep 098 | Mix:0.05 Noise:0.012 | Loss:0.5231 | Acc:82.24% | Best:82.32% | Time:28.8m
[S3] Ep 099 | Mix:0.05 Noise:0.012 | Loss:0.5429 | Acc:81.84% | Best:82.32% | Time:29.1m
[S3] Ep 100 | Mix:0.05 Noise:0.012 | Loss:0.5628 | Acc:81.80% | Best:82.32% | Time:29.4m
[S3] Ep 101 | Mix:0.05 Noise:0.012 | Loss:0.5385 | Acc:82.24% | Best:82.32% | Time:29.7m
[S3] Ep 102 | Mix:0.05 Noise:0.012 | Loss:0.5141 | Acc:81.88% | Best:82.32% | Time:29.9m
[S3] Ep 103 | Mix:0.05 Noise:0.012 | Loss:0.5805 | Acc:82.36% | Best:82.36% | Time:30.2m
[S3] Ep 104 | Mix:0.05 Noise:0.012 | Loss:0.6456 | Acc:82.00% | Best:82.36% | Time:30.5m
[S3] Ep 105 | Mix:0.05 Noise:0.012 | Loss:0.5385 | Acc:82.32% | Best:82.36% | Time:30.8m
[S3] Ep 106 | Mix:0.05 Noise:0.012 | Loss:0.5579 | Acc:81.92% | Best:82.36% | Time:31.1m
[S3] Ep 107 | Mix:0.05 Noise:0.012 | Loss:0.6396 | Acc:81.96% | Best:82.36% | Time:31.4m
[S3] Ep 108 | Mix:0.05 Noise:0.012 | Loss:0.5823 | Acc:81.72% | Best:82.36% | Time:31.7m
[S3] Ep 109 | Mix:0.05 Noise:0.012 | Loss:0.5549 | Acc:81.96% | Best:82.36% | Time:32.0m
[S3] Ep 110 | Mix:0.05 Noise:0.012 | Loss:0.6352 | Acc:81.68% | Best:82.36% | Time:32.3m
[S3] Ep 111 | Mix:0.05 Noise:0.012 | Loss:0.5700 | Acc:82.24% | Best:82.36% | Time:32.6m
[S3] Ep 112 | Mix:0.05 Noise:0.012 | Loss:0.5827 | Acc:81.48% | Best:82.36% | Time:32.8m
[S3] Ep 113 | Mix:0.05 Noise:0.012 | Loss:0.6553 | Acc:81.88% | Best:82.36% | Time:33.1m
[S3] Ep 114 | Mix:0.05 Noise:0.012 | Loss:0.6719 | Acc:81.56% | Best:82.36% | Time:33.4m
[S3] Ep 115 | Mix:0.05 Noise:0.012 | Loss:0.5715 | Acc:81.72% | Best:82.36% | Time:33.7m
