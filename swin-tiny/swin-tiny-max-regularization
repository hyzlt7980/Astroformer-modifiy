import torch
import torch.nn as nn
import torch.optim as optim
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader, DistributedSampler
from torchvision import datasets, transforms
import timm
from timm.data import create_transform
from timm.data.mixup import Mixup
from timm.loss import SoftTargetCrossEntropy
from timm.scheduler import create_scheduler
import argparse
import time
import os
import logging
import sys

#put comment
# ==========================================
# 0. Êó•ÂøóÂ∑•ÂÖ∑ setup_logger
# ==========================================
def setup_logger(save_dir, rank):
    """
    ÈÖçÁΩÆ LoggerÔºöÂêåÊó∂ËæìÂá∫Âà∞Êñá‰ª∂ÂíåÊéßÂà∂Âè∞
    ‰ªÖÂú® rank=0 Êó∂ËøîÂõû logger ÂØπË±°ÔºåÂÖ∂‰ªñ rank ËøîÂõû None (‰∏çËæìÂá∫)
    """
    if rank != 0:
        return None
    
    logger = logging.getLogger("swin_cifar100")
    logger.setLevel(logging.INFO)
    logger.propagate = False # Èò≤Ê≠¢ÈáçÂ§çÊâìÂç∞

    # Ê†ºÂºè
    formatter = logging.Formatter(
        '[%(asctime)s] %(message)s', 
        datefmt='%Y-%m-%d %H:%M:%S'
    )

    # 1. Êñá‰ª∂ËæìÂá∫ (File Handler)
    log_file = os.path.join(save_dir, 'training_log.txt')
    file_handler = logging.FileHandler(log_file, mode='a') # 'a' for append
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)

    # 2. ÊéßÂà∂Âè∞ËæìÂá∫ (Stream Handler)
    stream_handler = logging.StreamHandler(sys.stdout)
    stream_handler.setFormatter(formatter)
    logger.addHandler(stream_handler)

    return logger

# ==========================================
# 1. DDP ÁéØÂ¢ÉËÆæÁΩÆ
# ==========================================
def setup_distributed():
    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:
        rank = int(os.environ["RANK"])
        world_size = int(os.environ["WORLD_SIZE"])
        local_rank = int(os.environ["LOCAL_RANK"])
        
        torch.cuda.set_device(local_rank)
        dist.init_process_group(backend='nccl', init_method='env://', world_size=world_size, rank=rank)
        dist.barrier()
        return rank, local_rank, world_size
    else:
        print("Not using distributed mode")
        return 0, 0, 1

def cleanup():
    if dist.is_initialized():
        dist.destroy_process_group()

# ==========================================
# 2. ÈÖçÁΩÆÁ±ª
# ==========================================
class Config:
    # Êû∂ÊûÑ
    model_name = 'swin_tiny_patch4_window7_224'
    img_size = 64            # ‰∏äÈááÊ†∑Ëá≥ 64
    window_size = 4          # ÈÄÇÈÖç 64 ÁöÑÁ™óÂè£Â§ßÂ∞è
    patch_size = 4
    num_classes = 100
    
    # ËÆ≠ÁªÉ
    epochs = 300
    batch_size_per_gpu = 256  # Total Batch = 256 * 6 = 1536
    num_workers = 8
    
    # ‰ºòÂåñÂô®
    base_lr = 5e-4            # Âü∫Á°Ä LR
    min_lr = 5e-6
    weight_decay = 0.05
    warmup_epochs = 20
    
    # Â¢ûÂº∫
    mixup = 0.8
    cutmix = 1.0
    mixup_prob = 1.0
    label_smoothing = 0.1
    drop_path = 0.2
    
    save_dir = './checkpoints_swin64_ddp'

cfg = Config()

def main():
    # --- Step 1: Init DDP ---
    global_rank, local_rank, world_size = setup_distributed()
    device = torch.device("cuda", local_rank)

    # --- Step 2: Init Logger (Only Rank 0) ---
    if global_rank == 0:
        os.makedirs(cfg.save_dir, exist_ok=True)
    
    # Á°Æ‰øùÊñá‰ª∂Â§πÂàõÂª∫ÂêéÂÜçÂàùÂßãÂåñ logger
    if dist.is_initialized():
        dist.barrier()
        
    logger = setup_logger(cfg.save_dir, global_rank)

    if global_rank == 0:
        logger.info(f"üöÄ Start DDP Training on {world_size} GPUs")
        logger.info(f"Model: {cfg.model_name} | Img: {cfg.img_size} | Window: {cfg.window_size}")
        logger.info(f"Save Directory: {cfg.save_dir}")

    # --- Step 3: Compute LR ---
    total_batch_size = cfg.batch_size_per_gpu * world_size
    actual_lr = cfg.base_lr * (total_batch_size / 512.0)
    
    if global_rank == 0:
        logger.info(f"Total Batch Size: {total_batch_size}")
        logger.info(f"Scaled Learning Rate: {actual_lr:.6f}")

    # --- Step 4: Data Pipeline ---
    train_transform = create_transform(
        input_size=cfg.img_size,
        is_training=True,
        color_jitter=0.4,
        auto_augment='rand-m9-mstd0.5-inc1',
        interpolation='bicubic',
        re_prob=0.25, re_mode='pixel', re_count=1,
        mean=(0.5071, 0.4867, 0.4408), std=(0.2675, 0.2565, 0.2761)
    )

    val_transform = transforms.Compose([
        transforms.Resize((cfg.img_size, cfg.img_size), interpolation=transforms.InterpolationMode.BICUBIC),
        transforms.ToTensor(),
        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))
    ])

    # Âª∫ËÆÆÊèêÂâç‰∏ãËΩΩÂ•ΩÊï∞ÊçÆÔºåÈò≤Ê≠¢Â§öËøõÁ®ã‰∏ãËΩΩÂÜ≤Á™Å
    train_dataset = datasets.CIFAR100(root='./data', train=True, download=False, transform=train_transform)
    val_dataset = datasets.CIFAR100(root='./data', train=False, download=False, transform=val_transform)

    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=global_rank, shuffle=True)
    val_sampler = DistributedSampler(val_dataset, num_replicas=world_size, rank=global_rank, shuffle=False)

    train_loader = DataLoader(
        train_dataset, batch_size=cfg.batch_size_per_gpu, 
        shuffle=False, sampler=train_sampler,
        num_workers=cfg.num_workers, pin_memory=True, drop_last=True
    )
    val_loader = DataLoader(
        val_dataset, batch_size=cfg.batch_size_per_gpu, 
        sampler=val_sampler,
        num_workers=cfg.num_workers, pin_memory=True
    )

    mixup_fn = Mixup(
        mixup_alpha=cfg.mixup, cutmix_alpha=cfg.cutmix, 
        prob=cfg.mixup_prob, switch_prob=0.5, mode='batch',
        label_smoothing=cfg.label_smoothing, num_classes=cfg.num_classes
    )

    # --- Step 5: Model & DDP ---
    model = timm.create_model(
        cfg.model_name,
        pretrained=False,
        num_classes=cfg.num_classes,
        drop_path_rate=cfg.drop_path,
        img_size=cfg.img_size,
        window_size=cfg.window_size
    )
    model.to(device)
    model = DDP(model, device_ids=[local_rank], output_device=local_rank)

    # --- Step 6: Optim & Loss ---
    criterion_train = SoftTargetCrossEntropy()
    optimizer = optim.AdamW(model.parameters(), lr=actual_lr, weight_decay=cfg.weight_decay)

    scheduler_args = argparse.Namespace(
        sched='cosine', epochs=cfg.epochs, min_lr=cfg.min_lr, 
        warmup_epochs=cfg.warmup_epochs, warmup_lr=cfg.min_lr,
        cooldown_epochs=10, decay_rate=0.1
    )
    scheduler, _ = create_scheduler(scheduler_args, optimizer)

    # --- Step 7: Training Loop ---
    best_acc = 0.0
    start_time_global = time.time()

    for epoch in range(cfg.epochs):
        train_sampler.set_epoch(epoch)
        model.train()
        optimizer.zero_grad()
        local_loss = 0.0
        
        # Training
        for inputs, targets in train_loader:
            inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)
            
            if mixup_fn is not None:
                inputs, targets = mixup_fn(inputs, targets)
            
            outputs = model(inputs)
            loss = criterion_train(outputs, targets)
            
            loss.backward()
            nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)
            optimizer.step()
            optimizer.zero_grad()
            
            local_loss += loss.item()
        
        scheduler.step(epoch + 1)

        # Validation
        model.eval()
        correct_tensor = torch.tensor(0.0).to(device)
        total_tensor = torch.tensor(0.0).to(device)
        
        with torch.no_grad():
            for inputs, targets in val_loader:
                inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)
                outputs = model(inputs)
                _, predicted = outputs.max(1)
                total_tensor += targets.size(0)
                correct_tensor += predicted.eq(targets).sum().item()
        
        # DDP Reduce Stats
        dist.all_reduce(correct_tensor, op=dist.ReduceOp.SUM)
        dist.all_reduce(total_tensor, op=dist.ReduceOp.SUM)
        
        acc = 100. * correct_tensor.item() / total_tensor.item()
        
        # --- Logging (Rank 0 Only) ---
        if global_rank == 0:
            avg_loss = local_loss / len(train_loader) # Âè™ËÆ∞ÂΩï Rank 0 ÁöÑ loss ‰Ωú‰∏∫ÂèÇËÄÉ
            current_lr = optimizer.param_groups[0]['lr']
            
            save_msg = ""
            if acc > best_acc:
                best_acc = acc
                torch.save(model.module.state_dict(), os.path.join(cfg.save_dir, 'best_model.pth'))
                save_msg = "‚≠ê [Saved Best]"
            
            logger.info(
                f"Epoch [{epoch+1:03d}/{cfg.epochs}] "
                f"Loss: {avg_loss:.4f} | "
                f"Acc: {acc:.2f}% (Best: {best_acc:.2f}%) | "
                f"LR: {current_lr:.6f}{save_msg}"
            )

    # Finish
    if global_rank == 0:
        total_time = time.time() - start_time_global
        logger.info(f"‚úÖ Training Finished in {total_time/3600:.2f} hours.")
        logger.info(f"Final Best Accuracy: {best_acc:.2f}%")
        logger.info(f"Model saved to {cfg.save_dir}")

    cleanup()

if __name__ == '__main__':
    main()




torchrun --nproc_per_node=6 swin_tiny_max_regularzation.py
W0129 03:27:13.489000 2337 site-packages/torch/distributed/run.py:793]
W0129 03:27:13.489000 2337 site-packages/torch/distributed/run.py:793] *****************************************
W0129 03:27:13.489000 2337 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
W0129 03:27:13.489000 2337 site-packages/torch/distributed/run.py:793] *****************************************
[rank1]:[W129 03:27:21.712780741 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W129 03:27:21.717521018 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W129 03:27:21.719175840 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank5]:[W129 03:27:21.719737311 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank4]:[W129 03:27:21.719911746 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W129 03:27:21.721261184 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[2026-01-29 03:27:22] üöÄ Start DDP Training on 6 GPUs
[2026-01-29 03:27:22] Model: swin_tiny_patch4_window7_224 | Img: 64 | Window: 4
[2026-01-29 03:27:22] Save Directory: ./checkpoints_swin64_ddp
[2026-01-29 03:27:22] Total Batch Size: 1536
[2026-01-29 03:27:22] Scaled Learning Rate: 0.001500
[2026-01-29 03:27:42] Epoch [001/300] Loss: 4.6204 | Acc: 3.82% (Best: 3.82%) | LR: 0.000080‚≠ê [Saved Best]
[2026-01-29 03:27:47] Epoch [002/300] Loss: 4.5437 | Acc: 6.11% (Best: 6.11%) | LR: 0.000155‚≠ê [Saved Best]
[2026-01-29 03:27:52] Epoch [003/300] Loss: 4.5152 | Acc: 7.25% (Best: 7.25%) | LR: 0.000229‚≠ê [Saved Best]
[2026-01-29 03:27:57] Epoch [004/300] Loss: 4.4824 | Acc: 9.88% (Best: 9.88%) | LR: 0.000304‚≠ê [Saved Best]
[2026-01-29 03:28:02] Epoch [005/300] Loss: 4.4626 | Acc: 10.40% (Best: 10.40%) | LR: 0.000379‚≠ê [Saved Best]
[2026-01-29 03:28:07] Epoch [006/300] Loss: 4.4483 | Acc: 12.08% (Best: 12.08%) | LR: 0.000454‚≠ê [Saved Best]
[2026-01-29 03:28:11] Epoch [007/300] Loss: 4.4380 | Acc: 14.30% (Best: 14.30%) | LR: 0.000528‚≠ê [Saved Best]
[2026-01-29 03:28:16] Epoch [008/300] Loss: 4.3702 | Acc: 14.07% (Best: 14.30%) | LR: 0.000603
[2026-01-29 03:28:21] Epoch [009/300] Loss: 4.3635 | Acc: 15.42% (Best: 15.42%) | LR: 0.000678‚≠ê [Saved Best]
[2026-01-29 03:28:26] Epoch [010/300] Loss: 4.3926 | Acc: 16.36% (Best: 16.36%) | LR: 0.000753‚≠ê [Saved Best]
[2026-01-29 03:28:31] Epoch [011/300] Loss: 4.3450 | Acc: 17.12% (Best: 17.12%) | LR: 0.000827‚≠ê [Saved Best]
[2026-01-29 03:28:35] Epoch [012/300] Loss: 4.3072 | Acc: 16.95% (Best: 17.12%) | LR: 0.000902
[2026-01-29 03:28:41] Epoch [013/300] Loss: 4.3480 | Acc: 18.14% (Best: 18.14%) | LR: 0.000977‚≠ê [Saved Best]
[2026-01-29 03:28:46] Epoch [014/300] Loss: 4.3355 | Acc: 18.79% (Best: 18.79%) | LR: 0.001052‚≠ê [Saved Best]
[2026-01-29 03:28:51] Epoch [015/300] Loss: 4.2877 | Acc: 20.05% (Best: 20.05%) | LR: 0.001126‚≠ê [Saved Best]
[2026-01-29 03:28:55] Epoch [016/300] Loss: 4.2995 | Acc: 19.90% (Best: 20.05%) | LR: 0.001201
[2026-01-29 03:29:01] Epoch [017/300] Loss: 4.3245 | Acc: 20.89% (Best: 20.89%) | LR: 0.001276‚≠ê [Saved Best]
[2026-01-29 03:29:06] Epoch [018/300] Loss: 4.2634 | Acc: 20.96% (Best: 20.96%) | LR: 0.001350‚≠ê [Saved Best]
[2026-01-29 03:29:10] Epoch [019/300] Loss: 4.2919 | Acc: 23.19% (Best: 23.19%) | LR: 0.001425‚≠ê [Saved Best]
[2026-01-29 03:29:15] Epoch [020/300] Loss: 4.2482 | Acc: 22.18% (Best: 23.19%) | LR: 0.001484
[2026-01-29 03:29:20] Epoch [021/300] Loss: 4.2361 | Acc: 23.57% (Best: 23.57%) | LR: 0.001482‚≠ê [Saved Best]
[2026-01-29 03:29:25] Epoch [022/300] Loss: 4.2605 | Acc: 23.90% (Best: 23.90%) | LR: 0.001480‚≠ê [Saved Best]
[2026-01-29 03:29:30] Epoch [023/300] Loss: 4.1908 | Acc: 24.56% (Best: 24.56%) | LR: 0.001478‚≠ê [Saved Best]
[2026-01-29 03:29:35] Epoch [024/300] Loss: 4.2134 | Acc: 26.26% (Best: 26.26%) | LR: 0.001477‚≠ê [Saved Best]
[2026-01-29 03:29:40] Epoch [025/300] Loss: 4.2238 | Acc: 27.02% (Best: 27.02%) | LR: 0.001475‚≠ê [Saved Best]
[2026-01-29 03:29:45] Epoch [026/300] Loss: 4.2296 | Acc: 27.41% (Best: 27.41%) | LR: 0.001472‚≠ê [Saved Best]
[2026-01-29 03:29:50] Epoch [027/300] Loss: 4.1431 | Acc: 28.18% (Best: 28.18%) | LR: 0.001470‚≠ê [Saved Best]
[2026-01-29 03:29:54] Epoch [028/300] Loss: 4.1630 | Acc: 29.00% (Best: 29.00%) | LR: 0.001468‚≠ê [Saved Best]
[2026-01-29 03:29:59] Epoch [029/300] Loss: 4.1992 | Acc: 28.89% (Best: 29.00%) | LR: 0.001466
[2026-01-29 03:30:04] Epoch [030/300] Loss: 4.1478 | Acc: 29.65% (Best: 29.65%) | LR: 0.001463‚≠ê [Saved Best]
[2026-01-29 03:30:08] Epoch [031/300] Loss: 4.1590 | Acc: 29.47% (Best: 29.65%) | LR: 0.001461
[2026-01-29 03:30:14] Epoch [032/300] Loss: 4.1440 | Acc: 29.69% (Best: 29.69%) | LR: 0.001458‚≠ê [Saved Best]
[2026-01-29 03:30:19] Epoch [033/300] Loss: 4.1467 | Acc: 30.86% (Best: 30.86%) | LR: 0.001456‚≠ê [Saved Best]
[2026-01-29 03:30:24] Epoch [034/300] Loss: 4.0506 | Acc: 32.36% (Best: 32.36%) | LR: 0.001453‚≠ê [Saved Best]
[2026-01-29 03:30:28] Epoch [035/300] Loss: 4.1591 | Acc: 30.54% (Best: 32.36%) | LR: 0.001450
[2026-01-29 03:30:33] Epoch [036/300] Loss: 4.1806 | Acc: 31.65% (Best: 32.36%) | LR: 0.001448
[2026-01-29 03:30:38] Epoch [037/300] Loss: 4.0669 | Acc: 32.93% (Best: 32.93%) | LR: 0.001445‚≠ê [Saved Best]
[2026-01-29 03:30:43] Epoch [038/300] Loss: 4.0801 | Acc: 32.90% (Best: 32.93%) | LR: 0.001442
[2026-01-29 03:30:48] Epoch [039/300] Loss: 4.0407 | Acc: 34.64% (Best: 34.64%) | LR: 0.001439‚≠ê [Saved Best]
[2026-01-29 03:30:53] Epoch [040/300] Loss: 4.1061 | Acc: 35.22% (Best: 35.22%) | LR: 0.001435‚≠ê [Saved Best]
[2026-01-29 03:30:58] Epoch [041/300] Loss: 4.1006 | Acc: 35.33% (Best: 35.33%) | LR: 0.001432‚≠ê [Saved Best]
[2026-01-29 03:31:03] Epoch [042/300] Loss: 3.9361 | Acc: 35.69% (Best: 35.69%) | LR: 0.001429‚≠ê [Saved Best]
[2026-01-29 03:31:08] Epoch [043/300] Loss: 4.0869 | Acc: 36.84% (Best: 36.84%) | LR: 0.001425‚≠ê [Saved Best]
[2026-01-29 03:31:12] Epoch [044/300] Loss: 3.9158 | Acc: 36.10% (Best: 36.84%) | LR: 0.001422
[2026-01-29 03:31:17] Epoch [045/300] Loss: 4.0545 | Acc: 37.62% (Best: 37.62%) | LR: 0.001419‚≠ê [Saved Best]
[2026-01-29 03:31:22] Epoch [046/300] Loss: 4.0058 | Acc: 38.26% (Best: 38.26%) | LR: 0.001415‚≠ê [Saved Best]
[2026-01-29 03:31:27] Epoch [047/300] Loss: 3.9501 | Acc: 39.61% (Best: 39.61%) | LR: 0.001411‚≠ê [Saved Best]
[2026-01-29 03:31:32] Epoch [048/300] Loss: 4.0081 | Acc: 39.46% (Best: 39.61%) | LR: 0.001408
[2026-01-29 03:31:37] Epoch [049/300] Loss: 3.9527 | Acc: 37.65% (Best: 39.61%) | LR: 0.001404
[2026-01-29 03:31:43] Epoch [050/300] Loss: 3.9741 | Acc: 39.79% (Best: 39.79%) | LR: 0.001400‚≠ê [Saved Best]
[2026-01-29 03:31:48] Epoch [051/300] Loss: 3.9622 | Acc: 40.30% (Best: 40.30%) | LR: 0.001396‚≠ê [Saved Best]
[2026-01-29 03:31:53] Epoch [052/300] Loss: 3.9580 | Acc: 39.72% (Best: 40.30%) | LR: 0.001392
[2026-01-29 03:31:57] Epoch [053/300] Loss: 3.9771 | Acc: 39.86% (Best: 40.30%) | LR: 0.001388
[2026-01-29 03:32:02] Epoch [054/300] Loss: 3.9531 | Acc: 41.83% (Best: 41.83%) | LR: 0.001384‚≠ê [Saved Best]
[2026-01-29 03:32:07] Epoch [055/300] Loss: 3.9154 | Acc: 42.08% (Best: 42.08%) | LR: 0.001379‚≠ê [Saved Best]
[2026-01-29 03:32:12] Epoch [056/300] Loss: 3.9134 | Acc: 42.08% (Best: 42.08%) | LR: 0.001375
[2026-01-29 03:32:17] Epoch [057/300] Loss: 3.9466 | Acc: 42.47% (Best: 42.47%) | LR: 0.001371‚≠ê [Saved Best]
[2026-01-29 03:32:22] Epoch [058/300] Loss: 3.9400 | Acc: 42.88% (Best: 42.88%) | LR: 0.001366‚≠ê [Saved Best]
[2026-01-29 03:32:27] Epoch [059/300] Loss: 3.8325 | Acc: 43.39% (Best: 43.39%) | LR: 0.001362‚≠ê [Saved Best]
[2026-01-29 03:32:32] Epoch [060/300] Loss: 3.8551 | Acc: 43.54% (Best: 43.54%) | LR: 0.001357‚≠ê [Saved Best]
[2026-01-29 03:32:37] Epoch [061/300] Loss: 3.8712 | Acc: 43.63% (Best: 43.63%) | LR: 0.001353‚≠ê [Saved Best]
[2026-01-29 03:32:42] Epoch [062/300] Loss: 3.8065 | Acc: 43.48% (Best: 43.63%) | LR: 0.001348
[2026-01-29 03:32:47] Epoch [063/300] Loss: 3.8169 | Acc: 44.64% (Best: 44.64%) | LR: 0.001343‚≠ê [Saved Best]
[2026-01-29 03:32:52] Epoch [064/300] Loss: 3.8890 | Acc: 44.72% (Best: 44.72%) | LR: 0.001338‚≠ê [Saved Best]
[2026-01-29 03:32:56] Epoch [065/300] Loss: 3.8197 | Acc: 45.93% (Best: 45.93%) | LR: 0.001333‚≠ê [Saved Best]
[2026-01-29 03:33:01] Epoch [066/300] Loss: 3.8071 | Acc: 45.86% (Best: 45.93%) | LR: 0.001328
[2026-01-29 03:33:06] Epoch [067/300] Loss: 3.8819 | Acc: 46.10% (Best: 46.10%) | LR: 0.001323‚≠ê [Saved Best]
[2026-01-29 03:33:11] Epoch [068/300] Loss: 3.8819 | Acc: 46.45% (Best: 46.45%) | LR: 0.001318‚≠ê [Saved Best]
[2026-01-29 03:33:16] Epoch [069/300] Loss: 3.8231 | Acc: 48.05% (Best: 48.05%) | LR: 0.001313‚≠ê [Saved Best]
[2026-01-29 03:33:21] Epoch [070/300] Loss: 3.8218 | Acc: 46.49% (Best: 48.05%) | LR: 0.001308
[2026-01-29 03:33:25] Epoch [071/300] Loss: 3.8843 | Acc: 47.75% (Best: 48.05%) | LR: 0.001303
[2026-01-29 03:33:31] Epoch [072/300] Loss: 3.7040 | Acc: 48.75% (Best: 48.75%) | LR: 0.001297‚≠ê [Saved Best]
[2026-01-29 03:33:35] Epoch [073/300] Loss: 3.7584 | Acc: 48.03% (Best: 48.75%) | LR: 0.001292
[2026-01-29 03:33:40] Epoch [074/300] Loss: 3.7741 | Acc: 48.14% (Best: 48.75%) | LR: 0.001287
[2026-01-29 03:33:44] Epoch [075/300] Loss: 3.8194 | Acc: 48.46% (Best: 48.75%) | LR: 0.001281
[2026-01-29 03:33:49] Epoch [076/300] Loss: 3.7998 | Acc: 47.92% (Best: 48.75%) | LR: 0.001275
[2026-01-29 03:33:54] Epoch [077/300] Loss: 3.7493 | Acc: 48.69% (Best: 48.75%) | LR: 0.001270
[2026-01-29 03:33:59] Epoch [078/300] Loss: 3.8163 | Acc: 49.67% (Best: 49.67%) | LR: 0.001264‚≠ê [Saved Best]
[2026-01-29 03:34:04] Epoch [079/300] Loss: 3.7045 | Acc: 50.15% (Best: 50.15%) | LR: 0.001258‚≠ê [Saved Best]
[2026-01-29 03:34:09] Epoch [080/300] Loss: 3.6554 | Acc: 50.23% (Best: 50.23%) | LR: 0.001253‚≠ê [Saved Best]
[2026-01-29 03:34:14] Epoch [081/300] Loss: 3.7415 | Acc: 49.62% (Best: 50.23%) | LR: 0.001247
[2026-01-29 03:34:18] Epoch [082/300] Loss: 3.7548 | Acc: 50.11% (Best: 50.23%) | LR: 0.001241
[2026-01-29 03:34:24] Epoch [083/300] Loss: 3.7877 | Acc: 51.34% (Best: 51.34%) | LR: 0.001235‚≠ê [Saved Best]
[2026-01-29 03:34:29] Epoch [084/300] Loss: 3.7548 | Acc: 51.52% (Best: 51.52%) | LR: 0.001229‚≠ê [Saved Best]
[2026-01-29 03:34:33] Epoch [085/300] Loss: 3.8077 | Acc: 50.54% (Best: 51.52%) | LR: 0.001223
[2026-01-29 03:34:38] Epoch [086/300] Loss: 3.6956 | Acc: 51.92% (Best: 51.92%) | LR: 0.001217‚≠ê [Saved Best]
[2026-01-29 03:34:43] Epoch [087/300] Loss: 3.7305 | Acc: 51.88% (Best: 51.92%) | LR: 0.001211
[2026-01-29 03:34:48] Epoch [088/300] Loss: 3.7329 | Acc: 51.54% (Best: 51.92%) | LR: 0.001204
[2026-01-29 03:34:53] Epoch [089/300] Loss: 3.7096 | Acc: 52.41% (Best: 52.41%) | LR: 0.001198‚≠ê [Saved Best]
[2026-01-29 03:34:57] Epoch [090/300] Loss: 3.7846 | Acc: 52.33% (Best: 52.41%) | LR: 0.001192
[2026-01-29 03:35:02] Epoch [091/300] Loss: 3.6532 | Acc: 52.74% (Best: 52.74%) | LR: 0.001186‚≠ê [Saved Best]
[2026-01-29 03:35:07] Epoch [092/300] Loss: 3.7364 | Acc: 52.21% (Best: 52.74%) | LR: 0.001179
[2026-01-29 03:35:12] Epoch [093/300] Loss: 3.6747 | Acc: 52.73% (Best: 52.74%) | LR: 0.001173
[2026-01-29 03:35:17] Epoch [094/300] Loss: 3.6809 | Acc: 53.21% (Best: 53.21%) | LR: 0.001166‚≠ê [Saved Best]
[2026-01-29 03:35:22] Epoch [095/300] Loss: 3.5971 | Acc: 53.56% (Best: 53.56%) | LR: 0.001160‚≠ê [Saved Best]
[2026-01-29 03:35:27] Epoch [096/300] Loss: 3.7135 | Acc: 54.66% (Best: 54.66%) | LR: 0.001153‚≠ê [Saved Best]
[2026-01-29 03:35:32] Epoch [097/300] Loss: 3.6625 | Acc: 53.60% (Best: 54.66%) | LR: 0.001146
[2026-01-29 03:35:36] Epoch [098/300] Loss: 3.7503 | Acc: 54.36% (Best: 54.66%) | LR: 0.001140
[2026-01-29 03:35:41] Epoch [099/300] Loss: 3.6303 | Acc: 53.18% (Best: 54.66%) | LR: 0.001133
[2026-01-29 03:35:46] Epoch [100/300] Loss: 3.7002 | Acc: 54.50% (Best: 54.66%) | LR: 0.001126
[2026-01-29 03:35:51] Epoch [101/300] Loss: 3.6610 | Acc: 55.46% (Best: 55.46%) | LR: 0.001119‚≠ê [Saved Best]
[2026-01-29 03:35:55] Epoch [102/300] Loss: 3.6238 | Acc: 55.10% (Best: 55.46%) | LR: 0.001113
[2026-01-29 03:36:00] Epoch [103/300] Loss: 3.6442 | Acc: 55.06% (Best: 55.46%) | LR: 0.001106
[2026-01-29 03:36:05] Epoch [104/300] Loss: 3.6284 | Acc: 55.75% (Best: 55.75%) | LR: 0.001099‚≠ê [Saved Best]
[2026-01-29 03:36:10] Epoch [105/300] Loss: 3.5519 | Acc: 55.78% (Best: 55.78%) | LR: 0.001092‚≠ê [Saved Best]
[2026-01-29 03:36:15] Epoch [106/300] Loss: 3.7019 | Acc: 56.67% (Best: 56.67%) | LR: 0.001085‚≠ê [Saved Best]
[2026-01-29 03:36:20] Epoch [107/300] Loss: 3.7664 | Acc: 56.19% (Best: 56.67%) | LR: 0.001078
[2026-01-29 03:36:25] Epoch [108/300] Loss: 3.6106 | Acc: 57.12% (Best: 57.12%) | LR: 0.001071‚≠ê [Saved Best]
[2026-01-29 03:36:29] Epoch [109/300] Loss: 3.6202 | Acc: 56.98% (Best: 57.12%) | LR: 0.001064
[2026-01-29 03:36:34] Epoch [110/300] Loss: 3.5246 | Acc: 56.99% (Best: 57.12%) | LR: 0.001057
[2026-01-29 03:36:39] Epoch [111/300] Loss: 3.6776 | Acc: 57.59% (Best: 57.59%) | LR: 0.001049‚≠ê [Saved Best]
[2026-01-29 03:36:43] Epoch [112/300] Loss: 3.6657 | Acc: 57.24% (Best: 57.59%) | LR: 0.001042
[2026-01-29 03:36:49] Epoch [113/300] Loss: 3.5469 | Acc: 57.60% (Best: 57.60%) | LR: 0.001035‚≠ê [Saved Best]
[2026-01-29 03:36:54] Epoch [114/300] Loss: 3.5841 | Acc: 57.71% (Best: 57.71%) | LR: 0.001028‚≠ê [Saved Best]
[2026-01-29 03:36:58] Epoch [115/300] Loss: 3.7044 | Acc: 56.95% (Best: 57.71%) | LR: 0.001020
[2026-01-29 03:37:03] Epoch [116/300] Loss: 3.5258 | Acc: 57.61% (Best: 57.71%) | LR: 0.001013
[2026-01-29 03:37:08] Epoch [117/300] Loss: 3.4949 | Acc: 58.29% (Best: 58.29%) | LR: 0.001006‚≠ê [Saved Best]
[2026-01-29 03:37:13] Epoch [118/300] Loss: 3.5508 | Acc: 58.62% (Best: 58.62%) | LR: 0.000998‚≠ê [Saved Best]
[2026-01-29 03:37:18] Epoch [119/300] Loss: 3.5849 | Acc: 58.22% (Best: 58.62%) | LR: 0.000991
[2026-01-29 03:37:22] Epoch [120/300] Loss: 3.5815 | Acc: 58.11% (Best: 58.62%) | LR: 0.000983
[2026-01-29 03:37:27] Epoch [121/300] Loss: 3.5683 | Acc: 58.30% (Best: 58.62%) | LR: 0.000976
[2026-01-29 03:37:32] Epoch [122/300] Loss: 3.5122 | Acc: 59.29% (Best: 59.29%) | LR: 0.000969‚≠ê [Saved Best]
[2026-01-29 03:37:37] Epoch [123/300] Loss: 3.6062 | Acc: 59.09% (Best: 59.29%) | LR: 0.000961
[2026-01-29 03:37:42] Epoch [124/300] Loss: 3.5030 | Acc: 58.04% (Best: 59.29%) | LR: 0.000954
[2026-01-29 03:37:46] Epoch [125/300] Loss: 3.3476 | Acc: 59.25% (Best: 59.29%) | LR: 0.000946
[2026-01-29 03:37:51] Epoch [126/300] Loss: 3.4782 | Acc: 59.26% (Best: 59.29%) | LR: 0.000938
[2026-01-29 03:37:56] Epoch [127/300] Loss: 3.4785 | Acc: 59.40% (Best: 59.40%) | LR: 0.000931‚≠ê [Saved Best]
[2026-01-29 03:38:01] Epoch [128/300] Loss: 3.5619 | Acc: 60.33% (Best: 60.33%) | LR: 0.000923‚≠ê [Saved Best]
[2026-01-29 03:38:06] Epoch [129/300] Loss: 3.3776 | Acc: 59.89% (Best: 60.33%) | LR: 0.000916
[2026-01-29 03:38:10] Epoch [130/300] Loss: 3.4716 | Acc: 59.89% (Best: 60.33%) | LR: 0.000908
[2026-01-29 03:38:15] Epoch [131/300] Loss: 3.4836 | Acc: 60.17% (Best: 60.33%) | LR: 0.000900
[2026-01-29 03:38:20] Epoch [132/300] Loss: 3.5969 | Acc: 60.71% (Best: 60.71%) | LR: 0.000893‚≠ê [Saved Best]
[2026-01-29 03:38:25] Epoch [133/300] Loss: 3.4910 | Acc: 60.38% (Best: 60.71%) | LR: 0.000885
[2026-01-29 03:38:30] Epoch [134/300] Loss: 3.6250 | Acc: 60.90% (Best: 60.90%) | LR: 0.000877‚≠ê [Saved Best]
[2026-01-29 03:38:35] Epoch [135/300] Loss: 3.5136 | Acc: 60.51% (Best: 60.90%) | LR: 0.000869
[2026-01-29 03:38:40] Epoch [136/300] Loss: 3.4716 | Acc: 61.12% (Best: 61.12%) | LR: 0.000862‚≠ê [Saved Best]
[2026-01-29 03:38:45] Epoch [137/300] Loss: 3.4991 | Acc: 61.62% (Best: 61.62%) | LR: 0.000854‚≠ê [Saved Best]
[2026-01-29 03:38:49] Epoch [138/300] Loss: 3.4028 | Acc: 61.17% (Best: 61.62%) | LR: 0.000846
[2026-01-29 03:38:54] Epoch [139/300] Loss: 3.5307 | Acc: 61.03% (Best: 61.62%) | LR: 0.000838
[2026-01-29 03:38:58] Epoch [140/300] Loss: 3.5029 | Acc: 61.52% (Best: 61.62%) | LR: 0.000831
[2026-01-29 03:39:03] Epoch [141/300] Loss: 3.3650 | Acc: 62.02% (Best: 62.02%) | LR: 0.000823‚≠ê [Saved Best]
[2026-01-29 03:39:08] Epoch [142/300] Loss: 3.4784 | Acc: 62.00% (Best: 62.02%) | LR: 0.000815
[2026-01-29 03:39:13] Epoch [143/300] Loss: 3.4691 | Acc: 61.98% (Best: 62.02%) | LR: 0.000807
[2026-01-29 03:39:18] Epoch [144/300] Loss: 3.4969 | Acc: 62.13% (Best: 62.13%) | LR: 0.000799‚≠ê [Saved Best]
[2026-01-29 03:39:23] Epoch [145/300] Loss: 3.5052 | Acc: 62.46% (Best: 62.46%) | LR: 0.000792‚≠ê [Saved Best]
[2026-01-29 03:39:28] Epoch [146/300] Loss: 3.4125 | Acc: 61.71% (Best: 62.46%) | LR: 0.000784
[2026-01-29 03:39:33] Epoch [147/300] Loss: 3.2951 | Acc: 62.55% (Best: 62.55%) | LR: 0.000776‚≠ê [Saved Best]
[2026-01-29 03:39:38] Epoch [148/300] Loss: 3.3930 | Acc: 63.06% (Best: 63.06%) | LR: 0.000768‚≠ê [Saved Best]
[2026-01-29 03:39:42] Epoch [149/300] Loss: 3.3524 | Acc: 62.37% (Best: 63.06%) | LR: 0.000760
[2026-01-29 03:39:47] Epoch [150/300] Loss: 3.3077 | Acc: 63.90% (Best: 63.90%) | LR: 0.000753‚≠ê [Saved Best]
[2026-01-29 03:39:52] Epoch [151/300] Loss: 3.4503 | Acc: 63.68% (Best: 63.90%) | LR: 0.000745
[2026-01-29 03:39:56] Epoch [152/300] Loss: 3.3654 | Acc: 62.88% (Best: 63.90%) | LR: 0.000737
[2026-01-29 03:40:01] Epoch [153/300] Loss: 3.3207 | Acc: 63.59% (Best: 63.90%) | LR: 0.000729
[2026-01-29 03:40:06] Epoch [154/300] Loss: 3.3929 | Acc: 63.85% (Best: 63.90%) | LR: 0.000721
[2026-01-29 03:40:11] Epoch [155/300] Loss: 3.3843 | Acc: 64.45% (Best: 64.45%) | LR: 0.000713‚≠ê [Saved Best]
[2026-01-29 03:40:16] Epoch [156/300] Loss: 3.3641 | Acc: 64.00% (Best: 64.45%) | LR: 0.000706
[2026-01-29 03:40:21] Epoch [157/300] Loss: 3.3114 | Acc: 64.59% (Best: 64.59%) | LR: 0.000698‚≠ê [Saved Best]
[2026-01-29 03:40:25] Epoch [158/300] Loss: 3.3818 | Acc: 63.98% (Best: 64.59%) | LR: 0.000690
[2026-01-29 03:40:30] Epoch [159/300] Loss: 3.4005 | Acc: 64.23% (Best: 64.59%) | LR: 0.000682
[2026-01-29 03:40:35] Epoch [160/300] Loss: 3.4486 | Acc: 64.41% (Best: 64.59%) | LR: 0.000674
[2026-01-29 03:40:39] Epoch [161/300] Loss: 3.3617 | Acc: 64.51% (Best: 64.59%) | LR: 0.000667
[2026-01-29 03:40:44] Epoch [162/300] Loss: 3.3533 | Acc: 64.17% (Best: 64.59%) | LR: 0.000659
[2026-01-29 03:40:49] Epoch [163/300] Loss: 3.2722 | Acc: 64.26% (Best: 64.59%) | LR: 0.000651
[2026-01-29 03:40:54] Epoch [164/300] Loss: 3.3677 | Acc: 64.33% (Best: 64.59%) | LR: 0.000643
[2026-01-29 03:40:59] Epoch [165/300] Loss: 3.2389 | Acc: 65.22% (Best: 65.22%) | LR: 0.000636‚≠ê [Saved Best]
[2026-01-29 03:41:03] Epoch [166/300] Loss: 3.4472 | Acc: 64.57% (Best: 65.22%) | LR: 0.000628
[2026-01-29 03:41:08] Epoch [167/300] Loss: 3.5050 | Acc: 64.67% (Best: 65.22%) | LR: 0.000620
[2026-01-29 03:41:13] Epoch [168/300] Loss: 3.4709 | Acc: 64.96% (Best: 65.22%) | LR: 0.000612
[2026-01-29 03:41:18] Epoch [169/300] Loss: 3.3233 | Acc: 65.34% (Best: 65.34%) | LR: 0.000605‚≠ê [Saved Best]
[2026-01-29 03:41:22] Epoch [170/300] Loss: 3.3542 | Acc: 64.98% (Best: 65.34%) | LR: 0.000597
[2026-01-29 03:41:27] Epoch [171/300] Loss: 3.3016 | Acc: 65.03% (Best: 65.34%) | LR: 0.000589
[2026-01-29 03:41:33] Epoch [172/300] Loss: 3.4391 | Acc: 66.12% (Best: 66.12%) | LR: 0.000582‚≠ê [Saved Best]
[2026-01-29 03:41:37] Epoch [173/300] Loss: 3.3121 | Acc: 66.06% (Best: 66.12%) | LR: 0.000574
[2026-01-29 03:41:42] Epoch [174/300] Loss: 3.3435 | Acc: 65.72% (Best: 66.12%) | LR: 0.000567
[2026-01-29 03:41:47] Epoch [175/300] Loss: 3.2637 | Acc: 65.41% (Best: 66.12%) | LR: 0.000559
[2026-01-29 03:41:52] Epoch [176/300] Loss: 3.2712 | Acc: 65.36% (Best: 66.12%) | LR: 0.000551
[2026-01-29 03:41:56] Epoch [177/300] Loss: 3.1999 | Acc: 65.79% (Best: 66.12%) | LR: 0.000544
[2026-01-29 03:42:01] Epoch [178/300] Loss: 3.1967 | Acc: 66.11% (Best: 66.12%) | LR: 0.000536
[2026-01-29 03:42:06] Epoch [179/300] Loss: 3.2574 | Acc: 66.06% (Best: 66.12%) | LR: 0.000529
[2026-01-29 03:42:10] Epoch [180/300] Loss: 3.3610 | Acc: 65.75% (Best: 66.12%) | LR: 0.000522
[2026-01-29 03:42:15] Epoch [181/300] Loss: 3.2989 | Acc: 65.82% (Best: 66.12%) | LR: 0.000514
[2026-01-29 03:42:20] Epoch [182/300] Loss: 3.3085 | Acc: 66.24% (Best: 66.24%) | LR: 0.000507‚≠ê [Saved Best]
[2026-01-29 03:42:25] Epoch [183/300] Loss: 3.2900 | Acc: 66.51% (Best: 66.51%) | LR: 0.000499‚≠ê [Saved Best]
[2026-01-29 03:42:30] Epoch [184/300] Loss: 3.3130 | Acc: 66.67% (Best: 66.67%) | LR: 0.000492‚≠ê [Saved Best]
[2026-01-29 03:42:35] Epoch [185/300] Loss: 3.1966 | Acc: 66.53% (Best: 66.67%) | LR: 0.000485
[2026-01-29 03:42:40] Epoch [186/300] Loss: 3.1514 | Acc: 66.71% (Best: 66.71%) | LR: 0.000477‚≠ê [Saved Best]
[2026-01-29 03:42:45] Epoch [187/300] Loss: 3.2889 | Acc: 66.74% (Best: 66.74%) | LR: 0.000470‚≠ê [Saved Best]
[2026-01-29 03:42:49] Epoch [188/300] Loss: 3.2379 | Acc: 66.67% (Best: 66.74%) | LR: 0.000463
[2026-01-29 03:42:54] Epoch [189/300] Loss: 3.2278 | Acc: 66.87% (Best: 66.87%) | LR: 0.000456‚≠ê [Saved Best]
[2026-01-29 03:42:59] Epoch [190/300] Loss: 3.2854 | Acc: 67.00% (Best: 67.00%) | LR: 0.000448‚≠ê [Saved Best]
[2026-01-29 03:43:04] Epoch [191/300] Loss: 3.2464 | Acc: 66.96% (Best: 67.00%) | LR: 0.000441
[2026-01-29 03:43:09] Epoch [192/300] Loss: 3.2961 | Acc: 67.72% (Best: 67.72%) | LR: 0.000434‚≠ê [Saved Best]
[2026-01-29 03:43:14] Epoch [193/300] Loss: 3.1059 | Acc: 67.31% (Best: 67.72%) | LR: 0.000427
[2026-01-29 03:43:19] Epoch [194/300] Loss: 3.2122 | Acc: 67.49% (Best: 67.72%) | LR: 0.000420
[2026-01-29 03:43:23] Epoch [195/300] Loss: 3.3983 | Acc: 67.35% (Best: 67.72%) | LR: 0.000413
[2026-01-29 03:43:28] Epoch [196/300] Loss: 3.1795 | Acc: 67.27% (Best: 67.72%) | LR: 0.000406
[2026-01-29 03:43:33] Epoch [197/300] Loss: 3.1789 | Acc: 67.41% (Best: 67.72%) | LR: 0.000399
[2026-01-29 03:43:37] Epoch [198/300] Loss: 3.2406 | Acc: 67.50% (Best: 67.72%) | LR: 0.000392
[2026-01-29 03:43:42] Epoch [199/300] Loss: 3.2217 | Acc: 67.40% (Best: 67.72%) | LR: 0.000386
[2026-01-29 03:43:47] Epoch [200/300] Loss: 3.4259 | Acc: 67.29% (Best: 67.72%) | LR: 0.000379
[2026-01-29 03:43:52] Epoch [201/300] Loss: 3.2998 | Acc: 67.80% (Best: 67.80%) | LR: 0.000372‚≠ê [Saved Best]
[2026-01-29 03:43:57] Epoch [202/300] Loss: 3.1253 | Acc: 68.02% (Best: 68.02%) | LR: 0.000365‚≠ê [Saved Best]
[2026-01-29 03:44:02] Epoch [203/300] Loss: 3.1983 | Acc: 68.05% (Best: 68.05%) | LR: 0.000359‚≠ê [Saved Best]
[2026-01-29 03:44:06] Epoch [204/300] Loss: 3.2939 | Acc: 67.83% (Best: 68.05%) | LR: 0.000352
[2026-01-29 03:44:11] Epoch [205/300] Loss: 3.2028 | Acc: 67.78% (Best: 68.05%) | LR: 0.000345
[2026-01-29 03:44:16] Epoch [206/300] Loss: 3.0955 | Acc: 67.86% (Best: 68.05%) | LR: 0.000339
[2026-01-29 03:44:21] Epoch [207/300] Loss: 3.3000 | Acc: 68.20% (Best: 68.20%) | LR: 0.000332‚≠ê [Saved Best]
[2026-01-29 03:44:26] Epoch [208/300] Loss: 3.1321 | Acc: 68.55% (Best: 68.55%) | LR: 0.000326‚≠ê [Saved Best]
[2026-01-29 03:44:30] Epoch [209/300] Loss: 3.2961 | Acc: 68.19% (Best: 68.55%) | LR: 0.000319
[2026-01-29 03:44:36] Epoch [210/300] Loss: 3.2240 | Acc: 68.84% (Best: 68.84%) | LR: 0.000313‚≠ê [Saved Best]
[2026-01-29 03:44:40] Epoch [211/300] Loss: 3.0704 | Acc: 68.52% (Best: 68.84%) | LR: 0.000307
[2026-01-29 03:44:45] Epoch [212/300] Loss: 3.0358 | Acc: 68.77% (Best: 68.84%) | LR: 0.000301
[2026-01-29 03:44:50] Epoch [213/300] Loss: 2.9786 | Acc: 68.63% (Best: 68.84%) | LR: 0.000294
[2026-01-29 03:44:54] Epoch [214/300] Loss: 3.2595 | Acc: 68.54% (Best: 68.84%) | LR: 0.000288
[2026-01-29 03:44:59] Epoch [215/300] Loss: 3.2869 | Acc: 69.09% (Best: 69.09%) | LR: 0.000282‚≠ê [Saved Best]
[2026-01-29 03:45:04] Epoch [216/300] Loss: 3.2574 | Acc: 69.03% (Best: 69.09%) | LR: 0.000276
[2026-01-29 03:45:08] Epoch [217/300] Loss: 3.2600 | Acc: 68.78% (Best: 69.09%) | LR: 0.000270
[2026-01-29 03:45:13] Epoch [218/300] Loss: 3.1377 | Acc: 68.85% (Best: 69.09%) | LR: 0.000264
[2026-01-29 03:45:18] Epoch [219/300] Loss: 3.3246 | Acc: 68.74% (Best: 69.09%) | LR: 0.000258
[2026-01-29 03:45:23] Epoch [220/300] Loss: 3.2300 | Acc: 69.26% (Best: 69.26%) | LR: 0.000252‚≠ê [Saved Best]
[2026-01-29 03:45:28] Epoch [221/300] Loss: 3.2023 | Acc: 69.22% (Best: 69.26%) | LR: 0.000247
[2026-01-29 03:45:32] Epoch [222/300] Loss: 3.1754 | Acc: 69.02% (Best: 69.26%) | LR: 0.000241
[2026-01-29 03:45:37] Epoch [223/300] Loss: 3.2839 | Acc: 69.15% (Best: 69.26%) | LR: 0.000235
[2026-01-29 03:45:42] Epoch [224/300] Loss: 3.1912 | Acc: 69.47% (Best: 69.47%) | LR: 0.000230‚≠ê [Saved Best]
[2026-01-29 03:45:47] Epoch [225/300] Loss: 3.1544 | Acc: 69.47% (Best: 69.47%) | LR: 0.000224
[2026-01-29 03:45:52] Epoch [226/300] Loss: 3.2241 | Acc: 69.61% (Best: 69.61%) | LR: 0.000218‚≠ê [Saved Best]
[2026-01-29 03:45:56] Epoch [227/300] Loss: 3.3001 | Acc: 69.08% (Best: 69.61%) | LR: 0.000213
[2026-01-29 03:46:01] Epoch [228/300] Loss: 3.1281 | Acc: 69.58% (Best: 69.61%) | LR: 0.000208
[2026-01-29 03:46:06] Epoch [229/300] Loss: 3.1782 | Acc: 69.49% (Best: 69.61%) | LR: 0.000202
[2026-01-29 03:46:11] Epoch [230/300] Loss: 3.1150 | Acc: 69.64% (Best: 69.64%) | LR: 0.000197‚≠ê [Saved Best]
[2026-01-29 03:46:16] Epoch [231/300] Loss: 3.2446 | Acc: 69.40% (Best: 69.64%) | LR: 0.000192
[2026-01-29 03:46:21] Epoch [232/300] Loss: 3.0266 | Acc: 69.67% (Best: 69.67%) | LR: 0.000187‚≠ê [Saved Best]
[2026-01-29 03:46:25] Epoch [233/300] Loss: 3.1367 | Acc: 69.52% (Best: 69.67%) | LR: 0.000182
[2026-01-29 03:46:30] Epoch [234/300] Loss: 3.1748 | Acc: 69.34% (Best: 69.67%) | LR: 0.000177
[2026-01-29 03:46:35] Epoch [235/300] Loss: 3.2094 | Acc: 69.75% (Best: 69.75%) | LR: 0.000172‚≠ê [Saved Best]
[2026-01-29 03:46:40] Epoch [236/300] Loss: 3.1525 | Acc: 69.89% (Best: 69.89%) | LR: 0.000167‚≠ê [Saved Best]
[2026-01-29 03:46:44] Epoch [237/300] Loss: 3.2497 | Acc: 69.57% (Best: 69.89%) | LR: 0.000162
[2026-01-29 03:46:49] Epoch [238/300] Loss: 3.1830 | Acc: 69.87% (Best: 69.89%) | LR: 0.000157
[2026-01-29 03:46:54] Epoch [239/300] Loss: 3.0422 | Acc: 69.95% (Best: 69.95%) | LR: 0.000152‚≠ê [Saved Best]
[2026-01-29 03:46:59] Epoch [240/300] Loss: 3.1415 | Acc: 69.95% (Best: 69.95%) | LR: 0.000148
[2026-01-29 03:47:04] Epoch [241/300] Loss: 3.1368 | Acc: 69.97% (Best: 69.97%) | LR: 0.000143‚≠ê [Saved Best]
[2026-01-29 03:47:09] Epoch [242/300] Loss: 2.9753 | Acc: 70.19% (Best: 70.19%) | LR: 0.000139‚≠ê [Saved Best]
[2026-01-29 03:47:13] Epoch [243/300] Loss: 3.1483 | Acc: 69.73% (Best: 70.19%) | LR: 0.000134
[2026-01-29 03:47:18] Epoch [244/300] Loss: 3.1277 | Acc: 70.32% (Best: 70.32%) | LR: 0.000130‚≠ê [Saved Best]
[2026-01-29 03:47:23] Epoch [245/300] Loss: 3.0823 | Acc: 70.19% (Best: 70.32%) | LR: 0.000126
[2026-01-29 03:47:28] Epoch [246/300] Loss: 3.2087 | Acc: 69.92% (Best: 70.32%) | LR: 0.000121
[2026-01-29 03:47:33] Epoch [247/300] Loss: 3.1229 | Acc: 69.96% (Best: 70.32%) | LR: 0.000117
[2026-01-29 03:47:37] Epoch [248/300] Loss: 3.1494 | Acc: 70.24% (Best: 70.32%) | LR: 0.000113
[2026-01-29 03:47:42] Epoch [249/300] Loss: 3.1101 | Acc: 70.33% (Best: 70.33%) | LR: 0.000109‚≠ê [Saved Best]
[2026-01-29 03:47:47] Epoch [250/300] Loss: 2.8950 | Acc: 70.32% (Best: 70.33%) | LR: 0.000105
[2026-01-29 03:47:52] Epoch [251/300] Loss: 3.1374 | Acc: 70.52% (Best: 70.52%) | LR: 0.000101‚≠ê [Saved Best]
[2026-01-29 03:47:57] Epoch [252/300] Loss: 3.1062 | Acc: 70.52% (Best: 70.52%) | LR: 0.000097
[2026-01-29 03:48:01] Epoch [253/300] Loss: 2.9451 | Acc: 70.42% (Best: 70.52%) | LR: 0.000094
[2026-01-29 03:48:06] Epoch [254/300] Loss: 3.0585 | Acc: 70.17% (Best: 70.52%) | LR: 0.000090
[2026-01-29 03:48:11] Epoch [255/300] Loss: 3.2087 | Acc: 70.38% (Best: 70.52%) | LR: 0.000086
[2026-01-29 03:48:16] Epoch [256/300] Loss: 3.1199 | Acc: 70.30% (Best: 70.52%) | LR: 0.000083
[2026-01-29 03:48:20] Epoch [257/300] Loss: 3.0723 | Acc: 70.15% (Best: 70.52%) | LR: 0.000080
[2026-01-29 03:48:25] Epoch [258/300] Loss: 3.0194 | Acc: 70.32% (Best: 70.52%) | LR: 0.000076
[2026-01-29 03:48:30] Epoch [259/300] Loss: 3.1289 | Acc: 70.30% (Best: 70.52%) | LR: 0.000073
[2026-01-29 03:48:35] Epoch [260/300] Loss: 3.1633 | Acc: 70.62% (Best: 70.62%) | LR: 0.000070‚≠ê [Saved Best]
[2026-01-29 03:48:40] Epoch [261/300] Loss: 3.0086 | Acc: 70.57% (Best: 70.62%) | LR: 0.000066
[2026-01-29 03:48:44] Epoch [262/300] Loss: 3.0118 | Acc: 70.52% (Best: 70.62%) | LR: 0.000063
[2026-01-29 03:48:49] Epoch [263/300] Loss: 3.1649 | Acc: 70.41% (Best: 70.62%) | LR: 0.000060
[2026-01-29 03:48:54] Epoch [264/300] Loss: 3.0686 | Acc: 70.45% (Best: 70.62%) | LR: 0.000057
[2026-01-29 03:48:59] Epoch [265/300] Loss: 3.1297 | Acc: 70.69% (Best: 70.69%) | LR: 0.000055‚≠ê [Saved Best]
[2026-01-29 03:49:04] Epoch [266/300] Loss: 3.0430 | Acc: 70.47% (Best: 70.69%) | LR: 0.000052
[2026-01-29 03:49:08] Epoch [267/300] Loss: 3.2278 | Acc: 70.62% (Best: 70.69%) | LR: 0.000049
[2026-01-29 03:49:13] Epoch [268/300] Loss: 3.0631 | Acc: 70.59% (Best: 70.69%) | LR: 0.000047
[2026-01-29 03:49:18] Epoch [269/300] Loss: 3.0036 | Acc: 70.49% (Best: 70.69%) | LR: 0.000044
[2026-01-29 03:49:23] Epoch [270/300] Loss: 3.1240 | Acc: 70.70% (Best: 70.70%) | LR: 0.000042‚≠ê [Saved Best]
[2026-01-29 03:49:28] Epoch [271/300] Loss: 3.0654 | Acc: 70.41% (Best: 70.70%) | LR: 0.000039
[2026-01-29 03:49:32] Epoch [272/300] Loss: 3.0764 | Acc: 70.38% (Best: 70.70%) | LR: 0.000037
[2026-01-29 03:49:37] Epoch [273/300] Loss: 3.0105 | Acc: 70.55% (Best: 70.70%) | LR: 0.000035
[2026-01-29 03:49:42] Epoch [274/300] Loss: 3.0549 | Acc: 70.53% (Best: 70.70%) | LR: 0.000033
[2026-01-29 03:49:46] Epoch [275/300] Loss: 3.0263 | Acc: 70.63% (Best: 70.70%) | LR: 0.000030
[2026-01-29 03:49:52] Epoch [276/300] Loss: 3.0806 | Acc: 70.75% (Best: 70.75%) | LR: 0.000028‚≠ê [Saved Best]
[2026-01-29 03:49:56] Epoch [277/300] Loss: 3.0857 | Acc: 70.84% (Best: 70.84%) | LR: 0.000027‚≠ê [Saved Best]
[2026-01-29 03:50:01] Epoch [278/300] Loss: 3.0921 | Acc: 70.82% (Best: 70.84%) | LR: 0.000025
[2026-01-29 03:50:06] Epoch [279/300] Loss: 3.0681 | Acc: 70.87% (Best: 70.87%) | LR: 0.000023‚≠ê [Saved Best]
[2026-01-29 03:50:11] Epoch [280/300] Loss: 3.0353 | Acc: 70.54% (Best: 70.87%) | LR: 0.000021
[2026-01-29 03:50:15] Epoch [281/300] Loss: 3.0616 | Acc: 70.71% (Best: 70.87%) | LR: 0.000020
[2026-01-29 03:50:20] Epoch [282/300] Loss: 2.9242 | Acc: 70.72% (Best: 70.87%) | LR: 0.000018
[2026-01-29 03:50:25] Epoch [283/300] Loss: 3.2147 | Acc: 70.65% (Best: 70.87%) | LR: 0.000017
[2026-01-29 03:50:29] Epoch [284/300] Loss: 3.0238 | Acc: 70.71% (Best: 70.87%) | LR: 0.000015
[2026-01-29 03:50:34] Epoch [285/300] Loss: 2.9421 | Acc: 70.81% (Best: 70.87%) | LR: 0.000014
[2026-01-29 03:50:39] Epoch [286/300] Loss: 2.9854 | Acc: 70.74% (Best: 70.87%) | LR: 0.000013
[2026-01-29 03:50:44] Epoch [287/300] Loss: 3.1179 | Acc: 70.75% (Best: 70.87%) | LR: 0.000012
[2026-01-29 03:50:49] Epoch [288/300] Loss: 2.8777 | Acc: 70.77% (Best: 70.87%) | LR: 0.000011
[2026-01-29 03:50:53] Epoch [289/300] Loss: 3.1884 | Acc: 70.75% (Best: 70.87%) | LR: 0.000010
[2026-01-29 03:50:58] Epoch [290/300] Loss: 3.0194 | Acc: 70.78% (Best: 70.87%) | LR: 0.000009
[2026-01-29 03:51:03] Epoch [291/300] Loss: 2.9501 | Acc: 70.82% (Best: 70.87%) | LR: 0.000008
[2026-01-29 03:51:07] Epoch [292/300] Loss: 3.1486 | Acc: 70.77% (Best: 70.87%) | LR: 0.000008
[2026-01-29 03:51:12] Epoch [293/300] Loss: 2.9746 | Acc: 70.76% (Best: 70.87%) | LR: 0.000007
[2026-01-29 03:51:17] Epoch [294/300] Loss: 3.1299 | Acc: 70.73% (Best: 70.87%) | LR: 0.000006
[2026-01-29 03:51:22] Epoch [295/300] Loss: 2.8760 | Acc: 70.73% (Best: 70.87%) | LR: 0.000006
[2026-01-29 03:51:26] Epoch [296/300] Loss: 3.1553 | Acc: 70.77% (Best: 70.87%) | LR: 0.000006
[2026-01-29 03:51:31] Epoch [297/300] Loss: 3.1448 | Acc: 70.76% (Best: 70.87%) | LR: 0.000005
[2026-01-29 03:51:36] Epoch [298/300] Loss: 3.0839 | Acc: 70.79% (Best: 70.87%) | LR: 0.000005
[2026-01-29 03:51:40] Epoch [299/300] Loss: 3.0223 | Acc: 70.82% (Best: 70.87%) | LR: 0.000005
[2026-01-29 03:51:45] Epoch [300/300] Loss: 3.0077 | Acc: 70.85% (Best: 70.87%) | LR: 0.000005
[2026-01-29 03:51:45] ‚úÖ Training Finished in 0.41 hours.
[2026-01-29 03:51:45] Final Best Accuracy: 70.87%
[2026-01-29 03:51:45] Model saved to ./checkpoints_swin64_ddp
