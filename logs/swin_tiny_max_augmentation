(base) root@0e53c625a7a5:~# vim test.py
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import timm
from timm.data.mixup import Mixup
from timm.loss import SoftTargetCrossEntropy
from timm.scheduler import CosineLRScheduler
import time

# ================= 配置区域 (针对 64x64 调整) =================
# 分辨率变小了，计算量变小了，Batch Size 可以开得非常大
# 4张 4090 可以轻松跑 512 甚至 1024，这里保守写 256
BATCH_SIZE = 256
EPOCHS = 300
LR = 1e-3         # Batch变大，学习率可以稍微给大一点
WEIGHT_DECAY = 0.05

# === 核心修改：分辨率设为 64 ===
IMG_SIZE = 64

# ================= 设备配置 =================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ================= 数据增强 (满血版) =================
# 即使是 64x64，Mixup 依然是必须的
train_transform = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)), # 拉伸到 64
    transforms.RandomHorizontalFlip(),
    transforms.RandAugment(num_ops=2, magnitude=9),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

test_transform = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# 如果你本地没有数据集，download=True 会自动下载
train_dataset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=train_transform)
test_dataset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=test_transform)

"test.py" 140L, 4538C                                                                                                                     1,1          顶端
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import timm
from timm.data.mixup import Mixup
from timm.loss import SoftTargetCrossEntropy
from timm.scheduler import CosineLRScheduler
import time

# ================= 配置区域 (针对 64x64 调整) =================
# 分辨率变小了，计算量变小了，Batch Size 可以开得非常大
# 4张 4090 可以轻松跑 512 甚至 1024，这里保守写 256
BATCH_SIZE = 256
EPOCHS = 300
LR = 1e-3         # Batch变大，学习率可以稍微给大一点
WEIGHT_DECAY = 0.05

# === 核心修改：分辨率设为 64 ===
IMG_SIZE = 64

# ================= 设备配置 =================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ================= 数据增强 (满血版) =================
# 即使是 64x64，Mixup 依然是必须的
train_transform = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)), # 拉伸到 64
    transforms.RandomHorizontalFlip(),
    transforms.RandAugment(num_ops=2, magnitude=9),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

test_transform = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# 如果你本地没有数据集，download=True 会自动下载
train_dataset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=train_transform)
test_dataset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=test_transform)

"test.py" 140L, 4538C                                                                                                                     1,1          顶端
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import timm
from timm.data.mixup import Mixup
from timm.loss import SoftTargetCrossEntropy
from timm.scheduler import CosineLRScheduler
import time

# ================= 配置区域 (针对 64x64 调整) =================
# 分辨率变小了，计算量变小了，Batch Size 可以开得非常大
# 4张 4090 可以轻松跑 512 甚至 1024，这里保守写 256
BATCH_SIZE = 256
EPOCHS = 300
LR = 1e-3         # Batch变大，学习率可以稍微给大一点
WEIGHT_DECAY = 0.05

# === 核心修改：分辨率设为 64 ===
IMG_SIZE = 64

# ================= 设备配置 =================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ================= 数据增强 (满血版) =================
# 即使是 64x64，Mixup 依然是必须的
train_transform = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)), # 拉伸到 64
    transforms.RandomHorizontalFlip(),
    transforms.RandAugment(num_ops=2, magnitude=9),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

test_transform = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# 如果你本地没有数据集，download=True 会自动下载
train_dataset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=train_transform)
test_dataset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=test_transform)

# num_workers 设为 8 或 16，防止 GPU 等 CPU
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8, pin_memory=True)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=8, pin_memory=True)

-- 插入 --                                                                                                                                              1,1          顶端
LR = 1e-3         # Batch变大，学习率可以稍微给大一点
WEIGHT_DECAY = 0.05

# === 核心修改：分辨率设为 64 ===
IMG_SIZE = 64

# ================= 设备配置 =================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ================= 数据增强 (满血版) =================
# 即使是 64x64，Mixup 依然是必须的
train_transform = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)), # 拉伸到 64
    transforms.RandomHorizontalFlip(),
    transforms.RandAugment(num_ops=2, magnitude=9),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

test_transform = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# 如果你本地没有数据集，download=True 会自动下载
train_dataset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=train_transform)
test_dataset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=test_transform)

# num_workers 设为 8 或 16，防止 GPU 等 CPU
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8, pin_memory=True)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=8, pin_memory=True)

# ================= Mixup 定义 =================
mixup_fn = Mixup(
    mixup_alpha=0.8,
    cutmix_alpha=1.0,
    prob=1.0,
    switch_prob=0.5,
    mode='batch',
    label_smoothing=0.1,
    num_classes=100
)

# ================= 模型定义 =================
print(f"Creating Swin Transformer (Tiny) with Input {IMG_SIZE}x{IMG_SIZE}...")

# 关键点：显式传入 img_size=64
# timm 会自动处理 window_size=7 和 feature_map=16 之间的对齐问题（自动Padding）
model = timm.create_model(
-- 插入 --
