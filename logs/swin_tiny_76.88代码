(base) root@0e53c625a7a5:~# vim test.py
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import timm
from timm.data.mixup import Mixup
from timm.loss import SoftTargetCrossEntropy
from timm.scheduler import CosineLRScheduler
import time

# ================= 1. 配置区域 =================
BATCH_SIZE = 256
EPOCHS = 300
LR = 1e-3
WEIGHT_DECAY = 0.05
IMG_SIZE = 64
WINDOW_SIZE = 4  # 针对 64x64 分辨率优化的窗口大小
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ================= 2. 数据增强 =================
train_transform = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.RandomHorizontalFlip(),
    transforms.RandAugment(num_ops=2, magnitude=9),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

test_transform = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

train_dataset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=train_transform)
test_dataset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=test_transform)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8, pin_memory=True)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=8, pin_memory=True)

# ================= 3. Mixup 定义 =================
mixup_fn = Mixup(
    mixup_alpha=0.8,
    cutmix_alpha=1.0,
    prob=1.0,
    switch_prob=0.5,
    mode='batch',
    label_smoothing=0.1,
    num_classes=100
)

# ================= 4. 模型初始化 =================
print(f"Creating Swin Transformer (Tiny) with Window Size {WINDOW_SIZE}...")
model = timm.create_model(
    'swin_tiny_patch4_window7_224', 
    pretrained=True, 
    num_classes=100, 
    img_size=IMG_SIZE,
    window_size=WINDOW_SIZE,  # 设置为 4
    drop_rate=0.1,
    drop_path_rate=0.1
)
model.to(DEVICE)

# ================= 5. 损失函数、优化器、调度器 =================
# 注意：使用了 Mixup 必须配合 SoftTargetCrossEntropy
criterion = SoftTargetCrossEntropy()
optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)

# 余弦退火调度器
scheduler = CosineLRScheduler(
    optimizer, 
    t_initial=EPOCHS, 
    lr_min=1e-5, 
    warmup_t=10, 
    warmup_lr_init=1e-6
)

# ================= 6. 训练与验证逻辑 =================

def train_one_epoch(epoch):
    model.train()
    running_loss = 0.0
    for batch_idx, (inputs, targets) in enumerate(train_loader):
        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)
        
        # 应用 Mixup/Cutmix
        inputs, targets = mixup_fn(inputs, targets)
        
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()
        
        running_loss += loss.item()
    
    # 每个 epoch 更新一次学习率
    scheduler.step(epoch)
    return running_loss / len(train_loader)

def validate():
    model.eval()
    correct = 0
    total = 0
    # 验证时不使用 Mixup，使用标准交叉熵（或仅计算准确率）
    with torch.no_grad():
        for inputs, targets in test_loader:
            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)
            outputs = model(inputs)
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()
    
    return 100. * correct / total

# ================= 7. 主循环 =================
print("Start Training...")
best_acc = 0.0

for epoch in range(EPOCHS):
    start_time = time.time()
    
    train_loss = train_one_epoch(epoch)
    
    # 每 10 轮测试一次，节省时间
    if (epoch + 1) % 10 == 0 or epoch == 0:
        test_acc = validate()
        if test_acc > best_acc:
            best_acc = test_acc
            torch.save(model.state_dict(), 'swin_tiny_cifar100_best.pth')
        
        print(f"Epoch [{epoch+1}/{EPOCHS}] Loss: {train_loss:.4f} Test Acc: {test_acc:.2f}% Time: {time.time()-start_time:.2f}s")
    else:
        print(f"Epoch [{epoch+1}/{EPOCHS}] Loss: {train_loss:.4f} Time: {time.time()-start_time:.2f}s")

print(f"Training Finished. Best Accuracy: {best_acc:.2f}%")
